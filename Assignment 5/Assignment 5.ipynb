{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = 'Train.txt'\n",
    "path_to_CV = 'Cross_Validation.txt'\n",
    "path_to_test = 'Test.txt'\n",
    "\n",
    "train_data = pd.read_csv(path_to_train, header = None, sep = '#|;', engine = 'python')\n",
    "CV_data = pd.read_csv(path_to_CV, header = None, sep = '#|;', engine = 'python')\n",
    "test_data = pd.read_csv(path_to_test, header = None, sep = '#|;', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data.iloc[:, 0])\n",
    "x_train = np.array(train_data.iloc[:, 1:])\n",
    "\n",
    "y_CV = np.array(CV_data.iloc[:, 0])\n",
    "x_CV = np.array(CV_data.iloc[:, 1:])\n",
    "\n",
    "y_test = np.array(test_data.iloc[:, 0])\n",
    "x_test = np.array(test_data.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "x_CV = scaler.transform(x_CV)\n",
    "\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(150,150,150), max_iter=200, alpha=0.0001,\n",
    "                     solver='adam', verbose=True,  random_state=22,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.84976923\n",
      "Iteration 2, loss = 0.13881454\n",
      "Iteration 3, loss = 0.06484710\n",
      "Iteration 4, loss = 0.03348883\n",
      "Iteration 5, loss = 0.01635386\n",
      "Iteration 6, loss = 0.00857231\n",
      "Iteration 7, loss = 0.00379405\n",
      "Iteration 8, loss = 0.00213314\n",
      "Iteration 9, loss = 0.00155387\n",
      "Iteration 10, loss = 0.00124641\n",
      "Iteration 11, loss = 0.00104129\n",
      "Iteration 12, loss = 0.00089430\n",
      "Iteration 13, loss = 0.00078169\n",
      "Iteration 14, loss = 0.00069139\n",
      "Iteration 15, loss = 0.00062254\n",
      "Iteration 16, loss = 0.00056747\n",
      "Iteration 17, loss = 0.00052387\n",
      "Iteration 18, loss = 0.00048018\n",
      "Iteration 19, loss = 0.00044784\n",
      "Iteration 20, loss = 0.00041991\n",
      "Iteration 21, loss = 0.00039530\n",
      "Iteration 22, loss = 0.00037536\n",
      "Iteration 23, loss = 0.00035696\n",
      "Iteration 24, loss = 0.00034120\n",
      "Iteration 25, loss = 0.00032740\n",
      "Iteration 26, loss = 0.00031556\n",
      "Iteration 27, loss = 0.00030485\n",
      "Iteration 28, loss = 0.00029487\n",
      "Iteration 29, loss = 0.00028617\n",
      "Iteration 30, loss = 0.00027826\n",
      "Iteration 31, loss = 0.00027144\n",
      "Iteration 32, loss = 0.00026521\n",
      "Iteration 33, loss = 0.00025962\n",
      "Iteration 34, loss = 0.00025467\n",
      "Iteration 35, loss = 0.00024989\n",
      "Iteration 36, loss = 0.00024588\n",
      "Iteration 37, loss = 0.00024175\n",
      "Iteration 38, loss = 0.00023840\n",
      "Iteration 39, loss = 0.00023505\n",
      "Iteration 40, loss = 0.00023203\n",
      "Iteration 41, loss = 0.00022929\n",
      "Iteration 42, loss = 0.00022689\n",
      "Iteration 43, loss = 0.00022440\n",
      "Iteration 44, loss = 0.00022234\n",
      "Iteration 45, loss = 0.00022046\n",
      "Iteration 46, loss = 0.00021853\n",
      "Iteration 47, loss = 0.00021691\n",
      "Iteration 48, loss = 0.00021529\n",
      "Iteration 49, loss = 0.00021387\n",
      "Iteration 50, loss = 0.00021249\n",
      "Iteration 51, loss = 0.00021127\n",
      "Iteration 52, loss = 0.00020999\n",
      "Iteration 53, loss = 0.00020888\n",
      "Iteration 54, loss = 0.00020794\n",
      "Iteration 55, loss = 0.00020695\n",
      "Iteration 56, loss = 0.00020607\n",
      "Iteration 57, loss = 0.00020513\n",
      "Iteration 58, loss = 0.00020435\n",
      "Iteration 59, loss = 0.00020352\n",
      "Iteration 60, loss = 0.00020282\n",
      "Iteration 61, loss = 0.00020213\n",
      "Iteration 62, loss = 0.00020150\n",
      "Iteration 63, loss = 0.00020083\n",
      "Iteration 64, loss = 0.00020027\n",
      "Iteration 65, loss = 0.00019968\n",
      "Iteration 66, loss = 0.00019916\n",
      "Iteration 67, loss = 0.00019866\n",
      "Iteration 68, loss = 0.00019812\n",
      "Iteration 69, loss = 0.00019761\n",
      "Iteration 70, loss = 0.00019715\n",
      "Iteration 71, loss = 0.00019666\n",
      "Iteration 72, loss = 0.00019623\n",
      "Iteration 73, loss = 0.00019575\n",
      "Iteration 74, loss = 0.00019532\n",
      "Iteration 75, loss = 0.00019487\n",
      "Iteration 76, loss = 0.00019442\n",
      "Iteration 77, loss = 0.00019398\n",
      "Iteration 78, loss = 0.00019354\n",
      "Iteration 79, loss = 0.00019310\n",
      "Iteration 80, loss = 0.00019267\n",
      "Iteration 81, loss = 0.00019219\n",
      "Iteration 82, loss = 0.00019174\n",
      "Iteration 83, loss = 0.00019125\n",
      "Iteration 84, loss = 0.00019078\n",
      "Iteration 85, loss = 0.00019031\n",
      "Iteration 86, loss = 0.00018980\n",
      "Iteration 87, loss = 0.00018932\n",
      "Iteration 88, loss = 0.00018877\n",
      "Iteration 89, loss = 0.00018825\n",
      "Iteration 90, loss = 0.00018769\n",
      "Iteration 91, loss = 0.00018713\n",
      "Iteration 92, loss = 0.00018656\n",
      "Iteration 93, loss = 0.00018595\n",
      "Iteration 94, loss = 0.00018532\n",
      "Iteration 95, loss = 0.00018468\n",
      "Iteration 96, loss = 0.00018402\n",
      "Iteration 97, loss = 0.00018336\n",
      "Iteration 98, loss = 0.00018263\n",
      "Iteration 99, loss = 0.00018191\n",
      "Iteration 100, loss = 0.00018116\n",
      "Iteration 101, loss = 0.00018039\n",
      "Iteration 102, loss = 0.00017957\n",
      "Iteration 103, loss = 0.00017876\n",
      "Iteration 104, loss = 0.00017788\n",
      "Iteration 105, loss = 0.00017702\n",
      "Iteration 106, loss = 0.00017608\n",
      "Iteration 107, loss = 0.00017514\n",
      "Iteration 108, loss = 0.00017414\n",
      "Iteration 109, loss = 0.00017312\n",
      "Iteration 110, loss = 0.00017211\n",
      "Iteration 111, loss = 0.00017102\n",
      "Iteration 112, loss = 0.00016994\n",
      "Iteration 113, loss = 0.00016879\n",
      "Iteration 114, loss = 0.00016762\n",
      "Iteration 115, loss = 0.00016639\n",
      "Iteration 116, loss = 0.00016515\n",
      "Iteration 117, loss = 0.00016391\n",
      "Iteration 118, loss = 0.00016261\n",
      "Iteration 119, loss = 0.00016117\n",
      "Iteration 120, loss = 0.00015982\n",
      "Iteration 121, loss = 0.00015838\n",
      "Iteration 122, loss = 0.00015696\n",
      "Iteration 123, loss = 0.00015549\n",
      "Iteration 124, loss = 0.00015399\n",
      "Iteration 125, loss = 0.00015243\n",
      "Iteration 126, loss = 0.00015082\n",
      "Iteration 127, loss = 0.00014930\n",
      "Iteration 128, loss = 0.00014758\n",
      "Iteration 129, loss = 0.00014613\n",
      "Iteration 130, loss = 0.23349848\n",
      "Iteration 131, loss = 0.02000048\n",
      "Iteration 132, loss = 0.00620597\n",
      "Iteration 133, loss = 0.00164316\n",
      "Iteration 134, loss = 0.00074726\n",
      "Iteration 135, loss = 0.00056001\n",
      "Iteration 136, loss = 0.00047633\n",
      "Iteration 137, loss = 0.00042585\n",
      "Iteration 138, loss = 0.00038911\n",
      "Iteration 139, loss = 0.00036181\n",
      "Iteration 140, loss = 0.00033998\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(150, 150, 150), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=22, shuffle=True, solver='adam',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.14344957\n",
      "Iteration 2, loss = 2.49779927\n",
      "Iteration 3, loss = 2.04118346\n",
      "Iteration 4, loss = 1.64041217\n",
      "Iteration 5, loss = 1.25372524\n",
      "Iteration 6, loss = 0.94126640\n",
      "Iteration 7, loss = 0.72417715\n",
      "Iteration 8, loss = 0.54106577\n",
      "Iteration 9, loss = 0.41591324\n",
      "Iteration 10, loss = 0.34347897\n",
      "Iteration 11, loss = 0.26178882\n",
      "Iteration 12, loss = 0.22433117\n",
      "Iteration 13, loss = 0.18756613\n",
      "Iteration 14, loss = 0.14355950\n",
      "Iteration 15, loss = 0.11063382\n",
      "Iteration 16, loss = 0.08474401\n",
      "Iteration 17, loss = 0.06776675\n",
      "Iteration 18, loss = 0.05111701\n",
      "Iteration 19, loss = 0.04512973\n",
      "Iteration 20, loss = 0.04428725\n",
      "Iteration 21, loss = 0.03395877\n",
      "Iteration 22, loss = 0.02704634\n",
      "Iteration 23, loss = 0.02351722\n",
      "Iteration 24, loss = 0.02025200\n",
      "Iteration 25, loss = 0.01864246\n",
      "Iteration 26, loss = 0.01615368\n",
      "Iteration 27, loss = 0.01477279\n",
      "Iteration 28, loss = 0.01322771\n",
      "Iteration 29, loss = 0.01218614\n",
      "Iteration 30, loss = 0.01152515\n",
      "Iteration 31, loss = 0.01078980\n",
      "Iteration 32, loss = 0.01123569\n",
      "Iteration 33, loss = 0.00915982\n",
      "Iteration 34, loss = 0.00855554\n",
      "Iteration 35, loss = 0.00815424\n",
      "Iteration 36, loss = 0.00739555\n",
      "Iteration 37, loss = 0.00680425\n",
      "Iteration 38, loss = 0.00636887\n",
      "Iteration 39, loss = 0.00594223\n",
      "Iteration 40, loss = 0.00559209\n",
      "Iteration 41, loss = 0.00524827\n",
      "Iteration 42, loss = 0.00495530\n",
      "Iteration 43, loss = 0.00473079\n",
      "Iteration 44, loss = 0.00449936\n",
      "Iteration 45, loss = 0.00455560\n",
      "Iteration 46, loss = 0.00437996\n",
      "Iteration 47, loss = 0.00402348\n",
      "Iteration 48, loss = 0.00379625\n",
      "Iteration 49, loss = 0.00363400\n",
      "Iteration 50, loss = 0.00351286\n",
      "Iteration 51, loss = 0.00340121\n",
      "Iteration 52, loss = 0.00328601\n",
      "Iteration 53, loss = 0.00316806\n",
      "Iteration 54, loss = 0.00306003\n",
      "Iteration 55, loss = 0.00294482\n",
      "Iteration 56, loss = 0.00286618\n",
      "Iteration 57, loss = 0.00279501\n",
      "Iteration 58, loss = 0.00271310\n",
      "Iteration 59, loss = 0.00264980\n",
      "Iteration 60, loss = 0.00257354\n",
      "Iteration 61, loss = 0.00248307\n",
      "Iteration 62, loss = 0.00240953\n",
      "Iteration 63, loss = 0.00233563\n",
      "Iteration 64, loss = 0.00226767\n",
      "Iteration 65, loss = 0.00219715\n",
      "Iteration 66, loss = 0.00214176\n",
      "Iteration 67, loss = 0.00208786\n",
      "Iteration 68, loss = 0.00203459\n",
      "Iteration 69, loss = 0.00198125\n",
      "Iteration 70, loss = 0.00193556\n",
      "Iteration 71, loss = 0.00189070\n",
      "Iteration 72, loss = 0.00184340\n",
      "Iteration 73, loss = 0.00179986\n",
      "Iteration 74, loss = 0.00176413\n",
      "Iteration 75, loss = 0.00172547\n",
      "Iteration 76, loss = 0.00168785\n",
      "Iteration 77, loss = 0.00164875\n",
      "Iteration 78, loss = 0.00161297\n",
      "Iteration 79, loss = 0.00158158\n",
      "Iteration 80, loss = 0.00155041\n",
      "Iteration 81, loss = 0.00151991\n",
      "Iteration 82, loss = 0.00149242\n",
      "Iteration 83, loss = 0.00146793\n",
      "Iteration 84, loss = 0.00144674\n",
      "Iteration 85, loss = 0.00142121\n",
      "Iteration 86, loss = 0.00140271\n",
      "Iteration 87, loss = 0.00137740\n",
      "Iteration 88, loss = 0.00135232\n",
      "Iteration 89, loss = 0.00132607\n",
      "Iteration 90, loss = 0.00130068\n",
      "Iteration 91, loss = 0.00127757\n",
      "Iteration 92, loss = 0.00125652\n",
      "Iteration 93, loss = 0.00123891\n",
      "Iteration 94, loss = 0.00121763\n",
      "Iteration 95, loss = 0.00119491\n",
      "Iteration 96, loss = 0.00117688\n",
      "Iteration 97, loss = 0.00115550\n",
      "Iteration 98, loss = 0.00113692\n",
      "Iteration 99, loss = 0.00112219\n",
      "Iteration 100, loss = 0.00111098\n",
      "Iteration 101, loss = 0.00109593\n",
      "Iteration 102, loss = 0.00107823\n",
      "Iteration 103, loss = 0.00106023\n",
      "Iteration 104, loss = 0.00104338\n",
      "Iteration 105, loss = 0.00102711\n",
      "Iteration 106, loss = 0.00101270\n",
      "Iteration 107, loss = 0.00099725\n",
      "Iteration 108, loss = 0.00098462\n",
      "Iteration 109, loss = 0.00097221\n",
      "Iteration 110, loss = 0.00095944\n",
      "Iteration 111, loss = 0.00094711\n",
      "Iteration 112, loss = 0.00093490\n",
      "Iteration 113, loss = 0.00092406\n",
      "Iteration 114, loss = 0.00091139\n",
      "Iteration 115, loss = 0.00089902\n",
      "Iteration 116, loss = 0.00088631\n",
      "Iteration 117, loss = 0.00087526\n",
      "Iteration 118, loss = 0.00086414\n",
      "Iteration 119, loss = 0.00085269\n",
      "Iteration 120, loss = 0.00084168\n",
      "Iteration 121, loss = 0.00083217\n",
      "Iteration 122, loss = 0.00082379\n",
      "Iteration 123, loss = 0.00081430\n",
      "Iteration 124, loss = 0.00080485\n",
      "Iteration 125, loss = 0.00079622\n",
      "Iteration 126, loss = 0.00078767\n",
      "Iteration 127, loss = 0.00077915\n",
      "Iteration 128, loss = 0.00077015\n",
      "Iteration 129, loss = 0.00076063\n",
      "Iteration 130, loss = 0.00075168\n",
      "Iteration 131, loss = 0.00074405\n",
      "Iteration 132, loss = 0.00073729\n",
      "Iteration 133, loss = 0.00073069\n",
      "Iteration 134, loss = 0.00072463\n",
      "Iteration 135, loss = 0.00071789\n",
      "Iteration 136, loss = 0.00071166\n",
      "Iteration 137, loss = 0.00070540\n",
      "Iteration 138, loss = 0.00069830\n",
      "Iteration 139, loss = 0.00069052\n",
      "Iteration 140, loss = 0.00068397\n",
      "Iteration 141, loss = 0.00067804\n",
      "Iteration 142, loss = 0.00067167\n",
      "Iteration 143, loss = 0.00066592\n",
      "Iteration 144, loss = 0.00065998\n",
      "Iteration 145, loss = 0.00065415\n",
      "Iteration 146, loss = 0.00064837\n",
      "Iteration 147, loss = 0.00064187\n",
      "Iteration 148, loss = 0.00063798\n",
      "Iteration 149, loss = 0.00063286\n",
      "Iteration 150, loss = 0.00062955\n",
      "Iteration 151, loss = 0.00062694\n",
      "Iteration 152, loss = 0.00062199\n",
      "Iteration 153, loss = 0.00061604\n",
      "Iteration 154, loss = 0.00060867\n",
      "Iteration 155, loss = 0.00060128\n",
      "Iteration 156, loss = 0.00059386\n",
      "Iteration 157, loss = 0.00058772\n",
      "Iteration 158, loss = 0.00058305\n",
      "Iteration 159, loss = 0.00057861\n",
      "Iteration 160, loss = 0.00057453\n",
      "Iteration 161, loss = 0.00057010\n",
      "Iteration 162, loss = 0.00056603\n",
      "Iteration 163, loss = 0.00056225\n",
      "Iteration 164, loss = 0.00055892\n",
      "Iteration 165, loss = 0.00055501\n",
      "Iteration 166, loss = 0.00055117\n",
      "Iteration 167, loss = 0.00054669\n",
      "Iteration 168, loss = 0.00054135\n",
      "Iteration 169, loss = 0.00053720\n",
      "Iteration 170, loss = 0.00053328\n",
      "Iteration 171, loss = 0.00053009\n",
      "Iteration 172, loss = 0.00052690\n",
      "Iteration 173, loss = 0.00052356\n",
      "Iteration 174, loss = 0.00052053\n",
      "Iteration 175, loss = 0.00051710\n",
      "Iteration 176, loss = 0.00051394\n",
      "Iteration 177, loss = 0.00051079\n",
      "Iteration 178, loss = 0.00050745\n",
      "Iteration 179, loss = 0.00050493\n",
      "Iteration 180, loss = 0.00050221\n",
      "Iteration 181, loss = 0.00049960\n",
      "Iteration 182, loss = 0.00049662\n",
      "Iteration 183, loss = 0.00049352\n",
      "Iteration 184, loss = 0.00049061\n",
      "Iteration 185, loss = 0.00048767\n",
      "Iteration 186, loss = 0.00048507\n",
      "Iteration 187, loss = 0.00048247\n",
      "Iteration 188, loss = 0.00047945\n",
      "Iteration 189, loss = 0.00047639\n",
      "Iteration 190, loss = 0.00047344\n",
      "Iteration 191, loss = 0.00047048\n",
      "Iteration 192, loss = 0.00046771\n",
      "Iteration 193, loss = 0.00046469\n",
      "Iteration 194, loss = 0.00046199\n",
      "Iteration 195, loss = 0.00045941\n",
      "Iteration 196, loss = 0.00045709\n",
      "Iteration 197, loss = 0.00045489\n",
      "Iteration 198, loss = 0.00045287\n",
      "Iteration 199, loss = 0.00045104\n",
      "Iteration 200, loss = 0.00044881\n",
      "Iteration 1, loss = 3.14502305\n",
      "Iteration 2, loss = 2.48324967\n",
      "Iteration 3, loss = 2.01218538\n",
      "Iteration 4, loss = 1.58439527\n",
      "Iteration 5, loss = 1.19070927\n",
      "Iteration 6, loss = 0.87866233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 7, loss = 0.65803362\n",
      "Iteration 8, loss = 0.48814498\n",
      "Iteration 9, loss = 0.36494213\n",
      "Iteration 10, loss = 0.28412684\n",
      "Iteration 11, loss = 0.21133940\n",
      "Iteration 12, loss = 0.17268480\n",
      "Iteration 13, loss = 0.12994280\n",
      "Iteration 14, loss = 0.09950701\n",
      "Iteration 15, loss = 0.08704191\n",
      "Iteration 16, loss = 0.07345828\n",
      "Iteration 17, loss = 0.05389249\n",
      "Iteration 18, loss = 0.04162482\n",
      "Iteration 19, loss = 0.03858278\n",
      "Iteration 20, loss = 0.03574212\n",
      "Iteration 21, loss = 0.02591015\n",
      "Iteration 22, loss = 0.02154699\n",
      "Iteration 23, loss = 0.01867802\n",
      "Iteration 24, loss = 0.01583238\n",
      "Iteration 25, loss = 0.01400569\n",
      "Iteration 26, loss = 0.01237323\n",
      "Iteration 27, loss = 0.01130213\n",
      "Iteration 28, loss = 0.01047639\n",
      "Iteration 29, loss = 0.00960921\n",
      "Iteration 30, loss = 0.00871164\n",
      "Iteration 31, loss = 0.00797931\n",
      "Iteration 32, loss = 0.00728747\n",
      "Iteration 33, loss = 0.00670061\n",
      "Iteration 34, loss = 0.00640892\n",
      "Iteration 35, loss = 0.00623187\n",
      "Iteration 36, loss = 0.00576222\n",
      "Iteration 37, loss = 0.00542005\n",
      "Iteration 38, loss = 0.00526121\n",
      "Iteration 39, loss = 0.00486608\n",
      "Iteration 40, loss = 0.00450151\n",
      "Iteration 41, loss = 0.00424971\n",
      "Iteration 42, loss = 0.00404620\n",
      "Iteration 43, loss = 0.00387557\n",
      "Iteration 44, loss = 0.00371722\n",
      "Iteration 45, loss = 0.00366864\n",
      "Iteration 46, loss = 0.00353797\n",
      "Iteration 47, loss = 0.00332787\n",
      "Iteration 48, loss = 0.00315748\n",
      "Iteration 49, loss = 0.00301938\n",
      "Iteration 50, loss = 0.00291112\n",
      "Iteration 51, loss = 0.00281736\n",
      "Iteration 52, loss = 0.00272269\n",
      "Iteration 53, loss = 0.00262075\n",
      "Iteration 54, loss = 0.00252345\n",
      "Iteration 55, loss = 0.00243432\n",
      "Iteration 56, loss = 0.00236209\n",
      "Iteration 57, loss = 0.00229334\n",
      "Iteration 58, loss = 0.00222104\n",
      "Iteration 59, loss = 0.00216189\n",
      "Iteration 60, loss = 0.00209975\n",
      "Iteration 61, loss = 0.00204613\n",
      "Iteration 62, loss = 0.00199365\n",
      "Iteration 63, loss = 0.00193426\n",
      "Iteration 64, loss = 0.00188383\n",
      "Iteration 65, loss = 0.00182686\n",
      "Iteration 66, loss = 0.00178548\n",
      "Iteration 67, loss = 0.00174460\n",
      "Iteration 68, loss = 0.00170529\n",
      "Iteration 69, loss = 0.00166709\n",
      "Iteration 70, loss = 0.00163077\n",
      "Iteration 71, loss = 0.00158854\n",
      "Iteration 72, loss = 0.00155271\n",
      "Iteration 73, loss = 0.00152532\n",
      "Iteration 74, loss = 0.00149199\n",
      "Iteration 75, loss = 0.00145674\n",
      "Iteration 76, loss = 0.00141881\n",
      "Iteration 77, loss = 0.00138209\n",
      "Iteration 78, loss = 0.00135088\n",
      "Iteration 79, loss = 0.00132583\n",
      "Iteration 80, loss = 0.00130150\n",
      "Iteration 81, loss = 0.00127664\n",
      "Iteration 82, loss = 0.00125489\n",
      "Iteration 83, loss = 0.00123353\n",
      "Iteration 84, loss = 0.00121174\n",
      "Iteration 85, loss = 0.00118991\n",
      "Iteration 86, loss = 0.00116813\n",
      "Iteration 87, loss = 0.00114744\n",
      "Iteration 88, loss = 0.00112742\n",
      "Iteration 89, loss = 0.00110898\n",
      "Iteration 90, loss = 0.00109004\n",
      "Iteration 91, loss = 0.00107453\n",
      "Iteration 92, loss = 0.00105796\n",
      "Iteration 93, loss = 0.00104335\n",
      "Iteration 94, loss = 0.00102574\n",
      "Iteration 95, loss = 0.00100681\n",
      "Iteration 96, loss = 0.00099063\n",
      "Iteration 97, loss = 0.00097558\n",
      "Iteration 98, loss = 0.00096167\n",
      "Iteration 99, loss = 0.00095117\n",
      "Iteration 100, loss = 0.00094095\n",
      "Iteration 101, loss = 0.00092539\n",
      "Iteration 102, loss = 0.00090981\n",
      "Iteration 103, loss = 0.00089461\n",
      "Iteration 104, loss = 0.00088138\n",
      "Iteration 105, loss = 0.00086923\n",
      "Iteration 106, loss = 0.00085858\n",
      "Iteration 107, loss = 0.00084686\n",
      "Iteration 108, loss = 0.00083750\n",
      "Iteration 109, loss = 0.00082827\n",
      "Iteration 110, loss = 0.00081835\n",
      "Iteration 111, loss = 0.00080906\n",
      "Iteration 112, loss = 0.00079998\n",
      "Iteration 113, loss = 0.00079237\n",
      "Iteration 114, loss = 0.00078301\n",
      "Iteration 115, loss = 0.00077380\n",
      "Iteration 116, loss = 0.00076330\n",
      "Iteration 117, loss = 0.00075388\n",
      "Iteration 118, loss = 0.00074495\n",
      "Iteration 119, loss = 0.00073669\n",
      "Iteration 120, loss = 0.00072853\n",
      "Iteration 121, loss = 0.00072119\n",
      "Iteration 122, loss = 0.00071434\n",
      "Iteration 123, loss = 0.00070658\n",
      "Iteration 124, loss = 0.00069893\n",
      "Iteration 125, loss = 0.00069105\n",
      "Iteration 126, loss = 0.00068346\n",
      "Iteration 127, loss = 0.00067689\n",
      "Iteration 128, loss = 0.00067007\n",
      "Iteration 129, loss = 0.00066192\n",
      "Iteration 130, loss = 0.00065444\n",
      "Iteration 131, loss = 0.00064773\n",
      "Iteration 132, loss = 0.00064153\n",
      "Iteration 133, loss = 0.00063601\n",
      "Iteration 134, loss = 0.00063003\n",
      "Iteration 135, loss = 0.00062451\n",
      "Iteration 136, loss = 0.00061919\n",
      "Iteration 137, loss = 0.00061383\n",
      "Iteration 138, loss = 0.00060816\n",
      "Iteration 139, loss = 0.00060228\n",
      "Iteration 140, loss = 0.00059673\n",
      "Iteration 141, loss = 0.00059161\n",
      "Iteration 142, loss = 0.00058625\n",
      "Iteration 143, loss = 0.00058168\n",
      "Iteration 144, loss = 0.00057727\n",
      "Iteration 145, loss = 0.00057245\n",
      "Iteration 146, loss = 0.00056866\n",
      "Iteration 147, loss = 0.00056419\n",
      "Iteration 148, loss = 0.00055917\n",
      "Iteration 149, loss = 0.00055386\n",
      "Iteration 150, loss = 0.00055080\n",
      "Iteration 151, loss = 0.00054821\n",
      "Iteration 152, loss = 0.00054556\n",
      "Iteration 153, loss = 0.00054172\n",
      "Iteration 154, loss = 0.00053632\n",
      "Iteration 155, loss = 0.00053017\n",
      "Iteration 156, loss = 0.00052356\n",
      "Iteration 157, loss = 0.00051866\n",
      "Iteration 158, loss = 0.00051468\n",
      "Iteration 159, loss = 0.00051130\n",
      "Iteration 160, loss = 0.00050792\n",
      "Iteration 161, loss = 0.00050485\n",
      "Iteration 162, loss = 0.00050083\n",
      "Iteration 163, loss = 0.00049675\n",
      "Iteration 164, loss = 0.00049344\n",
      "Iteration 165, loss = 0.00048994\n",
      "Iteration 166, loss = 0.00048672\n",
      "Iteration 167, loss = 0.00048338\n",
      "Iteration 168, loss = 0.00047983\n",
      "Iteration 169, loss = 0.00047685\n",
      "Iteration 170, loss = 0.00047368\n",
      "Iteration 171, loss = 0.00047066\n",
      "Iteration 172, loss = 0.00046769\n",
      "Iteration 173, loss = 0.00046446\n",
      "Iteration 174, loss = 0.00046181\n",
      "Iteration 175, loss = 0.00045911\n",
      "Iteration 176, loss = 0.00045642\n",
      "Iteration 177, loss = 0.00045389\n",
      "Iteration 178, loss = 0.00045110\n",
      "Iteration 179, loss = 0.00044891\n",
      "Iteration 180, loss = 0.00044652\n",
      "Iteration 181, loss = 0.00044399\n",
      "Iteration 182, loss = 0.00044149\n",
      "Iteration 183, loss = 0.00043902\n",
      "Iteration 184, loss = 0.00043683\n",
      "Iteration 185, loss = 0.00043461\n",
      "Iteration 186, loss = 0.00043233\n",
      "Iteration 187, loss = 0.00043022\n",
      "Iteration 188, loss = 0.00042783\n",
      "Iteration 189, loss = 0.00042565\n",
      "Iteration 190, loss = 0.00042334\n",
      "Iteration 191, loss = 0.00042102\n",
      "Iteration 192, loss = 0.00041888\n",
      "Iteration 193, loss = 0.00041694\n",
      "Iteration 194, loss = 0.00041500\n",
      "Iteration 195, loss = 0.00041268\n",
      "Iteration 196, loss = 0.00041075\n",
      "Iteration 197, loss = 0.00040890\n",
      "Iteration 198, loss = 0.00040681\n",
      "Iteration 199, loss = 0.00040490\n",
      "Iteration 200, loss = 0.00040298\n",
      "Iteration 1, loss = 3.13577895\n",
      "Iteration 2, loss = 2.46757361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.99420916\n",
      "Iteration 4, loss = 1.56996711\n",
      "Iteration 5, loss = 1.19113454\n",
      "Iteration 6, loss = 0.86678846\n",
      "Iteration 7, loss = 0.64203600\n",
      "Iteration 8, loss = 0.46685513\n",
      "Iteration 9, loss = 0.35138976\n",
      "Iteration 10, loss = 0.26598059\n",
      "Iteration 11, loss = 0.19882744\n",
      "Iteration 12, loss = 0.15837609\n",
      "Iteration 13, loss = 0.13080034\n",
      "Iteration 14, loss = 0.09833284\n",
      "Iteration 15, loss = 0.08052229\n",
      "Iteration 16, loss = 0.06496774\n",
      "Iteration 17, loss = 0.04838812\n",
      "Iteration 18, loss = 0.03952306\n",
      "Iteration 19, loss = 0.03259305\n",
      "Iteration 20, loss = 0.02617325\n",
      "Iteration 21, loss = 0.02167020\n",
      "Iteration 22, loss = 0.01974457\n",
      "Iteration 23, loss = 0.01710699\n",
      "Iteration 24, loss = 0.01468240\n",
      "Iteration 25, loss = 0.01329068\n",
      "Iteration 26, loss = 0.01172301\n",
      "Iteration 27, loss = 0.01053365\n",
      "Iteration 28, loss = 0.01005779\n",
      "Iteration 29, loss = 0.00920918\n",
      "Iteration 30, loss = 0.00827664\n",
      "Iteration 31, loss = 0.00795619\n",
      "Iteration 32, loss = 0.00785852\n",
      "Iteration 33, loss = 0.00708500\n",
      "Iteration 34, loss = 0.00662476\n",
      "Iteration 35, loss = 0.00628675\n",
      "Iteration 36, loss = 0.00595430\n",
      "Iteration 37, loss = 0.00556053\n",
      "Iteration 38, loss = 0.00524060\n",
      "Iteration 39, loss = 0.00491257\n",
      "Iteration 40, loss = 0.00462562\n",
      "Iteration 41, loss = 0.00435971\n",
      "Iteration 42, loss = 0.00415273\n",
      "Iteration 43, loss = 0.00397899\n",
      "Iteration 44, loss = 0.00378460\n",
      "Iteration 45, loss = 0.00364095\n",
      "Iteration 46, loss = 0.00346883\n",
      "Iteration 47, loss = 0.00332557\n",
      "Iteration 48, loss = 0.00319211\n",
      "Iteration 49, loss = 0.00309107\n",
      "Iteration 50, loss = 0.00299874\n",
      "Iteration 51, loss = 0.00287874\n",
      "Iteration 52, loss = 0.00276487\n",
      "Iteration 53, loss = 0.00265648\n",
      "Iteration 54, loss = 0.00257951\n",
      "Iteration 55, loss = 0.00250552\n",
      "Iteration 56, loss = 0.00240193\n",
      "Iteration 57, loss = 0.00232435\n",
      "Iteration 58, loss = 0.00226014\n",
      "Iteration 59, loss = 0.00221476\n",
      "Iteration 60, loss = 0.00216285\n",
      "Iteration 61, loss = 0.00210464\n",
      "Iteration 62, loss = 0.00204183\n",
      "Iteration 63, loss = 0.00196188\n",
      "Iteration 64, loss = 0.00189218\n",
      "Iteration 65, loss = 0.00183158\n",
      "Iteration 66, loss = 0.00179141\n",
      "Iteration 67, loss = 0.00175000\n",
      "Iteration 68, loss = 0.00170072\n",
      "Iteration 69, loss = 0.00165297\n",
      "Iteration 70, loss = 0.00160704\n",
      "Iteration 71, loss = 0.00156214\n",
      "Iteration 72, loss = 0.00152595\n",
      "Iteration 73, loss = 0.00149290\n",
      "Iteration 74, loss = 0.00145851\n",
      "Iteration 75, loss = 0.00142437\n",
      "Iteration 76, loss = 0.00139184\n",
      "Iteration 77, loss = 0.00135941\n",
      "Iteration 78, loss = 0.00132929\n",
      "Iteration 79, loss = 0.00130349\n",
      "Iteration 80, loss = 0.00127862\n",
      "Iteration 81, loss = 0.00125420\n",
      "Iteration 82, loss = 0.00123326\n",
      "Iteration 83, loss = 0.00121075\n",
      "Iteration 84, loss = 0.00118742\n",
      "Iteration 85, loss = 0.00116444\n",
      "Iteration 86, loss = 0.00114627\n",
      "Iteration 87, loss = 0.00112535\n",
      "Iteration 88, loss = 0.00110521\n",
      "Iteration 89, loss = 0.00108345\n",
      "Iteration 90, loss = 0.00106527\n",
      "Iteration 91, loss = 0.00104830\n",
      "Iteration 92, loss = 0.00103192\n",
      "Iteration 93, loss = 0.00101287\n",
      "Iteration 94, loss = 0.00099852\n",
      "Iteration 95, loss = 0.00098285\n",
      "Iteration 96, loss = 0.00096851\n",
      "Iteration 97, loss = 0.00095337\n",
      "Iteration 98, loss = 0.00093892\n",
      "Iteration 99, loss = 0.00092378\n",
      "Iteration 100, loss = 0.00091060\n",
      "Iteration 101, loss = 0.00089594\n",
      "Iteration 102, loss = 0.00088274\n",
      "Iteration 103, loss = 0.00087017\n",
      "Iteration 104, loss = 0.00085730\n",
      "Iteration 105, loss = 0.00084501\n",
      "Iteration 106, loss = 0.00083248\n",
      "Iteration 107, loss = 0.00082105\n",
      "Iteration 108, loss = 0.00081043\n",
      "Iteration 109, loss = 0.00079984\n",
      "Iteration 110, loss = 0.00079014\n",
      "Iteration 111, loss = 0.00078194\n",
      "Iteration 112, loss = 0.00077284\n",
      "Iteration 113, loss = 0.00076473\n",
      "Iteration 114, loss = 0.00075558\n",
      "Iteration 115, loss = 0.00074731\n",
      "Iteration 116, loss = 0.00073845\n",
      "Iteration 117, loss = 0.00072971\n",
      "Iteration 118, loss = 0.00072017\n",
      "Iteration 119, loss = 0.00071108\n",
      "Iteration 120, loss = 0.00070187\n",
      "Iteration 121, loss = 0.00069537\n",
      "Iteration 122, loss = 0.00068911\n",
      "Iteration 123, loss = 0.00068126\n",
      "Iteration 124, loss = 0.00067310\n",
      "Iteration 125, loss = 0.00066529\n",
      "Iteration 126, loss = 0.00065892\n",
      "Iteration 127, loss = 0.00065170\n",
      "Iteration 128, loss = 0.00064506\n",
      "Iteration 129, loss = 0.00063742\n",
      "Iteration 130, loss = 0.00063038\n",
      "Iteration 131, loss = 0.00062433\n",
      "Iteration 132, loss = 0.00061773\n",
      "Iteration 133, loss = 0.00061229\n",
      "Iteration 134, loss = 0.00060722\n",
      "Iteration 135, loss = 0.00060237\n",
      "Iteration 136, loss = 0.00059744\n",
      "Iteration 137, loss = 0.00059197\n",
      "Iteration 138, loss = 0.00058654\n",
      "Iteration 139, loss = 0.00058043\n",
      "Iteration 140, loss = 0.00057442\n",
      "Iteration 141, loss = 0.00056899\n",
      "Iteration 142, loss = 0.00056370\n",
      "Iteration 143, loss = 0.00055909\n",
      "Iteration 144, loss = 0.00055436\n",
      "Iteration 145, loss = 0.00055026\n",
      "Iteration 146, loss = 0.00054717\n",
      "Iteration 147, loss = 0.00054253\n",
      "Iteration 148, loss = 0.00053712\n",
      "Iteration 149, loss = 0.00053221\n",
      "Iteration 150, loss = 0.00053015\n",
      "Iteration 151, loss = 0.00052917\n",
      "Iteration 152, loss = 0.00052676\n",
      "Iteration 153, loss = 0.00052202\n",
      "Iteration 154, loss = 0.00051620\n",
      "Iteration 155, loss = 0.00050866\n",
      "Iteration 156, loss = 0.00050283\n",
      "Iteration 157, loss = 0.00049891\n",
      "Iteration 158, loss = 0.00049594\n",
      "Iteration 159, loss = 0.00049278\n",
      "Iteration 160, loss = 0.00048939\n",
      "Iteration 161, loss = 0.00048590\n",
      "Iteration 162, loss = 0.00048219\n",
      "Iteration 163, loss = 0.00047863\n",
      "Iteration 164, loss = 0.00047574\n",
      "Iteration 165, loss = 0.00047239\n",
      "Iteration 166, loss = 0.00046912\n",
      "Iteration 167, loss = 0.00046588\n",
      "Iteration 168, loss = 0.00046250\n",
      "Iteration 169, loss = 0.00045972\n",
      "Iteration 170, loss = 0.00045672\n",
      "Iteration 171, loss = 0.00045370\n",
      "Iteration 172, loss = 0.00045088\n",
      "Iteration 173, loss = 0.00044824\n",
      "Iteration 174, loss = 0.00044563\n",
      "Iteration 175, loss = 0.00044313\n",
      "Iteration 176, loss = 0.00044068\n",
      "Iteration 177, loss = 0.00043831\n",
      "Iteration 178, loss = 0.00043571\n",
      "Iteration 179, loss = 0.00043332\n",
      "Iteration 180, loss = 0.00043102\n",
      "Iteration 181, loss = 0.00042865\n",
      "Iteration 182, loss = 0.00042612\n",
      "Iteration 183, loss = 0.00042367\n",
      "Iteration 184, loss = 0.00042137\n",
      "Iteration 185, loss = 0.00041908\n",
      "Iteration 186, loss = 0.00041703\n",
      "Iteration 187, loss = 0.00041504\n",
      "Iteration 188, loss = 0.00041278\n",
      "Iteration 189, loss = 0.00041049\n",
      "Iteration 190, loss = 0.00040852\n",
      "Iteration 191, loss = 0.00040653\n",
      "Iteration 192, loss = 0.00040457\n",
      "Iteration 193, loss = 0.00040282\n",
      "Iteration 194, loss = 0.00040089\n",
      "Iteration 195, loss = 0.00039893\n",
      "Iteration 196, loss = 0.00039703\n",
      "Iteration 197, loss = 0.00039523\n",
      "Iteration 198, loss = 0.00039335\n",
      "Iteration 199, loss = 0.00039133\n",
      "Iteration 200, loss = 0.00038949\n",
      "Iteration 1, loss = 3.12106550\n",
      "Iteration 2, loss = 2.46672636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 3, loss = 1.99904999\n",
      "Iteration 4, loss = 1.57285338\n",
      "Iteration 5, loss = 1.18336032\n",
      "Iteration 6, loss = 0.86254125\n",
      "Iteration 7, loss = 0.64258266\n",
      "Iteration 8, loss = 0.47302406\n",
      "Iteration 9, loss = 0.35819892\n",
      "Iteration 10, loss = 0.27760233\n",
      "Iteration 11, loss = 0.21390045\n",
      "Iteration 12, loss = 0.16325540\n",
      "Iteration 13, loss = 0.12604739\n",
      "Iteration 14, loss = 0.10153205\n",
      "Iteration 15, loss = 0.07909735\n",
      "Iteration 16, loss = 0.06386763\n",
      "Iteration 17, loss = 0.05174308\n",
      "Iteration 18, loss = 0.04344664\n",
      "Iteration 19, loss = 0.03442412\n",
      "Iteration 20, loss = 0.03138279\n",
      "Iteration 21, loss = 0.02342817\n",
      "Iteration 22, loss = 0.02085633\n",
      "Iteration 23, loss = 0.01722819\n",
      "Iteration 24, loss = 0.01470801\n",
      "Iteration 25, loss = 0.01345226\n",
      "Iteration 26, loss = 0.01240086\n",
      "Iteration 27, loss = 0.01104990\n",
      "Iteration 28, loss = 0.00994078\n",
      "Iteration 29, loss = 0.00904424\n",
      "Iteration 30, loss = 0.00833445\n",
      "Iteration 31, loss = 0.00773496\n",
      "Iteration 32, loss = 0.00724282\n",
      "Iteration 33, loss = 0.00675852\n",
      "Iteration 34, loss = 0.00654357\n",
      "Iteration 35, loss = 0.00599925\n",
      "Iteration 36, loss = 0.00553454\n",
      "Iteration 37, loss = 0.00515482\n",
      "Iteration 38, loss = 0.00484285\n",
      "Iteration 39, loss = 0.00456184\n",
      "Iteration 40, loss = 0.00432640\n",
      "Iteration 41, loss = 0.00411029\n",
      "Iteration 42, loss = 0.00391201\n",
      "Iteration 43, loss = 0.00385196\n",
      "Iteration 44, loss = 0.00372343\n",
      "Iteration 45, loss = 0.00353201\n",
      "Iteration 46, loss = 0.00333756\n",
      "Iteration 47, loss = 0.00319059\n",
      "Iteration 48, loss = 0.00306305\n",
      "Iteration 49, loss = 0.00294290\n",
      "Iteration 50, loss = 0.00284578\n",
      "Iteration 51, loss = 0.00273782\n",
      "Iteration 52, loss = 0.00264433\n",
      "Iteration 53, loss = 0.00255185\n",
      "Iteration 54, loss = 0.00248450\n",
      "Iteration 55, loss = 0.00241959\n",
      "Iteration 56, loss = 0.00233211\n",
      "Iteration 57, loss = 0.00228386\n",
      "Iteration 58, loss = 0.00222145\n",
      "Iteration 59, loss = 0.00215616\n",
      "Iteration 60, loss = 0.00209264\n",
      "Iteration 61, loss = 0.00203332\n",
      "Iteration 62, loss = 0.00196760\n",
      "Iteration 63, loss = 0.00191320\n",
      "Iteration 64, loss = 0.00186090\n",
      "Iteration 65, loss = 0.00180970\n",
      "Iteration 66, loss = 0.00175603\n",
      "Iteration 67, loss = 0.00171115\n",
      "Iteration 68, loss = 0.00166685\n",
      "Iteration 69, loss = 0.00162564\n",
      "Iteration 70, loss = 0.00158184\n",
      "Iteration 71, loss = 0.00154257\n",
      "Iteration 72, loss = 0.00150474\n",
      "Iteration 73, loss = 0.00147153\n",
      "Iteration 74, loss = 0.00143917\n",
      "Iteration 75, loss = 0.00140770\n",
      "Iteration 76, loss = 0.00137555\n",
      "Iteration 77, loss = 0.00134608\n",
      "Iteration 78, loss = 0.00131883\n",
      "Iteration 79, loss = 0.00129594\n",
      "Iteration 80, loss = 0.00127440\n",
      "Iteration 81, loss = 0.00124898\n",
      "Iteration 82, loss = 0.00122478\n",
      "Iteration 83, loss = 0.00120157\n",
      "Iteration 84, loss = 0.00117759\n",
      "Iteration 85, loss = 0.00115357\n",
      "Iteration 86, loss = 0.00113317\n",
      "Iteration 87, loss = 0.00111298\n",
      "Iteration 88, loss = 0.00109450\n",
      "Iteration 89, loss = 0.00107717\n",
      "Iteration 90, loss = 0.00105866\n",
      "Iteration 91, loss = 0.00104207\n",
      "Iteration 92, loss = 0.00102540\n",
      "Iteration 93, loss = 0.00100653\n",
      "Iteration 94, loss = 0.00098989\n",
      "Iteration 95, loss = 0.00097385\n",
      "Iteration 96, loss = 0.00095893\n",
      "Iteration 97, loss = 0.00094714\n",
      "Iteration 98, loss = 0.00093552\n",
      "Iteration 99, loss = 0.00092241\n",
      "Iteration 100, loss = 0.00090840\n",
      "Iteration 101, loss = 0.00089450\n",
      "Iteration 102, loss = 0.00088158\n",
      "Iteration 103, loss = 0.00086919\n",
      "Iteration 104, loss = 0.00085537\n",
      "Iteration 105, loss = 0.00084270\n",
      "Iteration 106, loss = 0.00083105\n",
      "Iteration 107, loss = 0.00081900\n",
      "Iteration 108, loss = 0.00080807\n",
      "Iteration 109, loss = 0.00079804\n",
      "Iteration 110, loss = 0.00078866\n",
      "Iteration 111, loss = 0.00078013\n",
      "Iteration 112, loss = 0.00077124\n",
      "Iteration 113, loss = 0.00076354\n",
      "Iteration 114, loss = 0.00075539\n",
      "Iteration 115, loss = 0.00074727\n",
      "Iteration 116, loss = 0.00073834\n",
      "Iteration 117, loss = 0.00072917\n",
      "Iteration 118, loss = 0.00071952\n",
      "Iteration 119, loss = 0.00071074\n",
      "Iteration 120, loss = 0.00070163\n",
      "Iteration 121, loss = 0.00069431\n",
      "Iteration 122, loss = 0.00068749\n",
      "Iteration 123, loss = 0.00068072\n",
      "Iteration 124, loss = 0.00067392\n",
      "Iteration 125, loss = 0.00066800\n",
      "Iteration 126, loss = 0.00065976\n",
      "Iteration 127, loss = 0.00065238\n",
      "Iteration 128, loss = 0.00064438\n",
      "Iteration 129, loss = 0.00063718\n",
      "Iteration 130, loss = 0.00063121\n",
      "Iteration 131, loss = 0.00062696\n",
      "Iteration 132, loss = 0.00062146\n",
      "Iteration 133, loss = 0.00061588\n",
      "Iteration 134, loss = 0.00060932\n",
      "Iteration 135, loss = 0.00060348\n",
      "Iteration 136, loss = 0.00059810\n",
      "Iteration 137, loss = 0.00059252\n",
      "Iteration 138, loss = 0.00058688\n",
      "Iteration 139, loss = 0.00058071\n",
      "Iteration 140, loss = 0.00057499\n",
      "Iteration 141, loss = 0.00056992\n",
      "Iteration 142, loss = 0.00056484\n",
      "Iteration 143, loss = 0.00056019\n",
      "Iteration 144, loss = 0.00055601\n",
      "Iteration 145, loss = 0.00055184\n",
      "Iteration 146, loss = 0.00054827\n",
      "Iteration 147, loss = 0.00054430\n",
      "Iteration 148, loss = 0.00053939\n",
      "Iteration 149, loss = 0.00053439\n",
      "Iteration 150, loss = 0.00053007\n",
      "Iteration 151, loss = 0.00052650\n",
      "Iteration 152, loss = 0.00052252\n",
      "Iteration 153, loss = 0.00051791\n",
      "Iteration 154, loss = 0.00051327\n",
      "Iteration 155, loss = 0.00050858\n",
      "Iteration 156, loss = 0.00050456\n",
      "Iteration 157, loss = 0.00050093\n",
      "Iteration 158, loss = 0.00049754\n",
      "Iteration 159, loss = 0.00049450\n",
      "Iteration 160, loss = 0.00049121\n",
      "Iteration 161, loss = 0.00048764\n",
      "Iteration 162, loss = 0.00048440\n",
      "Iteration 163, loss = 0.00048110\n",
      "Iteration 164, loss = 0.00047823\n",
      "Iteration 165, loss = 0.00047475\n",
      "Iteration 166, loss = 0.00047153\n",
      "Iteration 167, loss = 0.00046857\n",
      "Iteration 168, loss = 0.00046593\n",
      "Iteration 169, loss = 0.00046336\n",
      "Iteration 170, loss = 0.00046037\n",
      "Iteration 171, loss = 0.00045731\n",
      "Iteration 172, loss = 0.00045465\n",
      "Iteration 173, loss = 0.00045211\n",
      "Iteration 174, loss = 0.00044970\n",
      "Iteration 175, loss = 0.00044711\n",
      "Iteration 176, loss = 0.00044473\n",
      "Iteration 177, loss = 0.00044233\n",
      "Iteration 178, loss = 0.00043958\n",
      "Iteration 179, loss = 0.00043728\n",
      "Iteration 180, loss = 0.00043510\n",
      "Iteration 181, loss = 0.00043270\n",
      "Iteration 182, loss = 0.00043043\n",
      "Iteration 183, loss = 0.00042799\n",
      "Iteration 184, loss = 0.00042582\n",
      "Iteration 185, loss = 0.00042368\n",
      "Iteration 186, loss = 0.00042133\n",
      "Iteration 187, loss = 0.00041931\n",
      "Iteration 188, loss = 0.00041720\n",
      "Iteration 189, loss = 0.00041535\n",
      "Iteration 190, loss = 0.00041354\n",
      "Iteration 191, loss = 0.00041157\n",
      "Iteration 192, loss = 0.00040967\n",
      "Iteration 193, loss = 0.00040774\n",
      "Iteration 194, loss = 0.00040573\n",
      "Iteration 195, loss = 0.00040373\n",
      "Iteration 196, loss = 0.00040150\n",
      "Iteration 197, loss = 0.00039956\n",
      "Iteration 198, loss = 0.00039791\n",
      "Iteration 199, loss = 0.00039617\n",
      "Iteration 200, loss = 0.00039434\n",
      "Iteration 1, loss = 3.13185798\n",
      "Iteration 2, loss = 2.47757614\n",
      "Iteration 3, loss = 1.98642011\n",
      "Iteration 4, loss = 1.55401225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.16367555\n",
      "Iteration 6, loss = 0.84113570\n",
      "Iteration 7, loss = 0.63515440\n",
      "Iteration 8, loss = 0.45908230\n",
      "Iteration 9, loss = 0.34087722\n",
      "Iteration 10, loss = 0.27287889\n",
      "Iteration 11, loss = 0.20390922\n",
      "Iteration 12, loss = 0.16466906\n",
      "Iteration 13, loss = 0.12970738\n",
      "Iteration 14, loss = 0.09757304\n",
      "Iteration 15, loss = 0.07946281\n",
      "Iteration 16, loss = 0.06678591\n",
      "Iteration 17, loss = 0.05307080\n",
      "Iteration 18, loss = 0.04051491\n",
      "Iteration 19, loss = 0.03670170\n",
      "Iteration 20, loss = 0.03368638\n",
      "Iteration 21, loss = 0.02185983\n",
      "Iteration 22, loss = 0.02144309\n",
      "Iteration 23, loss = 0.01752925\n",
      "Iteration 24, loss = 0.01453787\n",
      "Iteration 25, loss = 0.01424422\n",
      "Iteration 26, loss = 0.01449477\n",
      "Iteration 27, loss = 0.01248529\n",
      "Iteration 28, loss = 0.01600768\n",
      "Iteration 29, loss = 0.01228425\n",
      "Iteration 30, loss = 0.00981823\n",
      "Iteration 31, loss = 0.00862428\n",
      "Iteration 32, loss = 0.00761899\n",
      "Iteration 33, loss = 0.00677204\n",
      "Iteration 34, loss = 0.00614934\n",
      "Iteration 35, loss = 0.00594394\n",
      "Iteration 36, loss = 0.00570550\n",
      "Iteration 37, loss = 0.00513992\n",
      "Iteration 38, loss = 0.00470424\n",
      "Iteration 39, loss = 0.00437473\n",
      "Iteration 40, loss = 0.00419908\n",
      "Iteration 41, loss = 0.00397168\n",
      "Iteration 42, loss = 0.00371867\n",
      "Iteration 43, loss = 0.00357348\n",
      "Iteration 44, loss = 0.00343644\n",
      "Iteration 45, loss = 0.00328723\n",
      "Iteration 46, loss = 0.00312276\n",
      "Iteration 47, loss = 0.00298637\n",
      "Iteration 48, loss = 0.00291331\n",
      "Iteration 49, loss = 0.00282283\n",
      "Iteration 50, loss = 0.00270256\n",
      "Iteration 51, loss = 0.00259409\n",
      "Iteration 52, loss = 0.00250442\n",
      "Iteration 53, loss = 0.00241422\n",
      "Iteration 54, loss = 0.00234133\n",
      "Iteration 55, loss = 0.00227791\n",
      "Iteration 56, loss = 0.00220508\n",
      "Iteration 57, loss = 0.00213924\n",
      "Iteration 58, loss = 0.00207702\n",
      "Iteration 59, loss = 0.00201553\n",
      "Iteration 60, loss = 0.00197350\n",
      "Iteration 61, loss = 0.00192603\n",
      "Iteration 62, loss = 0.00187390\n",
      "Iteration 63, loss = 0.00181962\n",
      "Iteration 64, loss = 0.00177381\n",
      "Iteration 65, loss = 0.00172689\n",
      "Iteration 66, loss = 0.00168796\n",
      "Iteration 67, loss = 0.00164543\n",
      "Iteration 68, loss = 0.00160451\n",
      "Iteration 69, loss = 0.00156249\n",
      "Iteration 70, loss = 0.00151999\n",
      "Iteration 71, loss = 0.00148333\n",
      "Iteration 72, loss = 0.00144972\n",
      "Iteration 73, loss = 0.00141918\n",
      "Iteration 74, loss = 0.00138790\n",
      "Iteration 75, loss = 0.00136077\n",
      "Iteration 76, loss = 0.00133189\n",
      "Iteration 77, loss = 0.00130620\n",
      "Iteration 78, loss = 0.00128078\n",
      "Iteration 79, loss = 0.00125852\n",
      "Iteration 80, loss = 0.00123631\n",
      "Iteration 81, loss = 0.00120987\n",
      "Iteration 82, loss = 0.00118793\n",
      "Iteration 83, loss = 0.00116754\n",
      "Iteration 84, loss = 0.00114765\n",
      "Iteration 85, loss = 0.00112324\n",
      "Iteration 86, loss = 0.00111211\n",
      "Iteration 87, loss = 0.00109772\n",
      "Iteration 88, loss = 0.00107938\n",
      "Iteration 89, loss = 0.00106012\n",
      "Iteration 90, loss = 0.00104227\n",
      "Iteration 91, loss = 0.00102622\n",
      "Iteration 92, loss = 0.00100600\n",
      "Iteration 93, loss = 0.00098353\n",
      "Iteration 94, loss = 0.00096575\n",
      "Iteration 95, loss = 0.00095044\n",
      "Iteration 96, loss = 0.00093567\n",
      "Iteration 97, loss = 0.00092208\n",
      "Iteration 98, loss = 0.00090785\n",
      "Iteration 99, loss = 0.00089478\n",
      "Iteration 100, loss = 0.00088168\n",
      "Iteration 101, loss = 0.00086867\n",
      "Iteration 102, loss = 0.00085546\n",
      "Iteration 103, loss = 0.00084152\n",
      "Iteration 104, loss = 0.00082848\n",
      "Iteration 105, loss = 0.00081576\n",
      "Iteration 106, loss = 0.00080462\n",
      "Iteration 107, loss = 0.00079345\n",
      "Iteration 108, loss = 0.00078285\n",
      "Iteration 109, loss = 0.00077346\n",
      "Iteration 110, loss = 0.00076420\n",
      "Iteration 111, loss = 0.00075511\n",
      "Iteration 112, loss = 0.00074674\n",
      "Iteration 113, loss = 0.00073718\n",
      "Iteration 114, loss = 0.00072959\n",
      "Iteration 115, loss = 0.00072292\n",
      "Iteration 116, loss = 0.00071543\n",
      "Iteration 117, loss = 0.00070644\n",
      "Iteration 118, loss = 0.00069783\n",
      "Iteration 119, loss = 0.00068895\n",
      "Iteration 120, loss = 0.00068001\n",
      "Iteration 121, loss = 0.00067292\n",
      "Iteration 122, loss = 0.00066574\n",
      "Iteration 123, loss = 0.00065882\n",
      "Iteration 124, loss = 0.00065267\n",
      "Iteration 125, loss = 0.00064710\n",
      "Iteration 126, loss = 0.00063963\n",
      "Iteration 127, loss = 0.00063170\n",
      "Iteration 128, loss = 0.00062415\n",
      "Iteration 129, loss = 0.00061704\n",
      "Iteration 130, loss = 0.00061135\n",
      "Iteration 131, loss = 0.00060602\n",
      "Iteration 132, loss = 0.00060006\n",
      "Iteration 133, loss = 0.00059514\n",
      "Iteration 134, loss = 0.00058933\n",
      "Iteration 135, loss = 0.00058438\n",
      "Iteration 136, loss = 0.00057942\n",
      "Iteration 137, loss = 0.00057366\n",
      "Iteration 138, loss = 0.00056806\n",
      "Iteration 139, loss = 0.00056269\n",
      "Iteration 140, loss = 0.00055773\n",
      "Iteration 141, loss = 0.00055253\n",
      "Iteration 142, loss = 0.00054773\n",
      "Iteration 143, loss = 0.00054391\n",
      "Iteration 144, loss = 0.00054012\n",
      "Iteration 145, loss = 0.00053550\n",
      "Iteration 146, loss = 0.00053047\n",
      "Iteration 147, loss = 0.00052676\n",
      "Iteration 148, loss = 0.00052247\n",
      "Iteration 149, loss = 0.00051736\n",
      "Iteration 150, loss = 0.00051239\n",
      "Iteration 151, loss = 0.00050813\n",
      "Iteration 152, loss = 0.00050425\n",
      "Iteration 153, loss = 0.00050030\n",
      "Iteration 154, loss = 0.00049605\n",
      "Iteration 155, loss = 0.00049153\n",
      "Iteration 156, loss = 0.00048790\n",
      "Iteration 157, loss = 0.00048443\n",
      "Iteration 158, loss = 0.00048144\n",
      "Iteration 159, loss = 0.00047813\n",
      "Iteration 160, loss = 0.00047480\n",
      "Iteration 161, loss = 0.00047161\n",
      "Iteration 162, loss = 0.00046893\n",
      "Iteration 163, loss = 0.00046648\n",
      "Iteration 164, loss = 0.00046405\n",
      "Iteration 165, loss = 0.00046061\n",
      "Iteration 166, loss = 0.00045796\n",
      "Iteration 167, loss = 0.00045525\n",
      "Iteration 168, loss = 0.00045236\n",
      "Iteration 169, loss = 0.00044963\n",
      "Iteration 170, loss = 0.00044663\n",
      "Iteration 171, loss = 0.00044354\n",
      "Iteration 172, loss = 0.00044063\n",
      "Iteration 173, loss = 0.00043807\n",
      "Iteration 174, loss = 0.00043572\n",
      "Iteration 175, loss = 0.00043343\n",
      "Iteration 176, loss = 0.00043107\n",
      "Iteration 177, loss = 0.00042881\n",
      "Iteration 178, loss = 0.00042646\n",
      "Iteration 179, loss = 0.00042438\n",
      "Iteration 180, loss = 0.00042210\n",
      "Iteration 181, loss = 0.00041995\n",
      "Iteration 182, loss = 0.00041775\n",
      "Iteration 183, loss = 0.00041555\n",
      "Iteration 184, loss = 0.00041354\n",
      "Iteration 185, loss = 0.00041155\n",
      "Iteration 186, loss = 0.00040953\n",
      "Iteration 187, loss = 0.00040760\n",
      "Iteration 188, loss = 0.00040569\n",
      "Iteration 189, loss = 0.00040376\n",
      "Iteration 190, loss = 0.00040171\n",
      "Iteration 191, loss = 0.00039969\n",
      "Iteration 192, loss = 0.00039767\n",
      "Iteration 193, loss = 0.00039579\n",
      "Iteration 194, loss = 0.00039381\n",
      "Iteration 195, loss = 0.00039208\n",
      "Iteration 196, loss = 0.00039032\n",
      "Iteration 197, loss = 0.00038855\n",
      "Iteration 198, loss = 0.00038699\n",
      "Iteration 199, loss = 0.00038538\n",
      "Iteration 200, loss = 0.00038353\n",
      "Cross-Validation score:  1.0346534653465347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = -1 * cross_val_score(clf, x_CV, y_CV, cv=5, scoring='neg_mean_absolute_error');\n",
    "print(\"Cross-Validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data accuracy:  1.0\n",
      "For Cross-validation: 0.9871287128712871\n",
      "For test data: 0.981169474727453\n",
      "[[34  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 50  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0 47  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 38  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 33  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 52  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 48  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 50  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 40  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 31  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 44  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 37  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0 36  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 28  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 49  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 42  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0 50  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 37  1  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 37  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 38  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        35\n",
      "           1       0.96      0.98      0.97        51\n",
      "           2       0.98      0.96      0.97        49\n",
      "           3       1.00      1.00      1.00        38\n",
      "           4       1.00      0.94      0.97        35\n",
      "           5       0.98      0.98      0.98        53\n",
      "           6       0.98      0.98      0.98        49\n",
      "           7       0.98      0.98      0.98        51\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        31\n",
      "          10       1.00      1.00      1.00        44\n",
      "          11       0.93      1.00      0.96        37\n",
      "          12       0.96      1.00      0.98        47\n",
      "          13       1.00      0.92      0.96        39\n",
      "          14       0.97      1.00      0.98        28\n",
      "          15       0.96      1.00      0.98        49\n",
      "          16       1.00      0.98      0.99        43\n",
      "          17       0.98      0.94      0.96        53\n",
      "          18       1.00      0.97      0.99        38\n",
      "          19       0.95      0.97      0.96        37\n",
      "          20       1.00      1.00      1.00        40\n",
      "          21       0.97      0.97      0.97        38\n",
      "          22       1.00      1.00      1.00        38\n",
      "          23       0.98      1.00      0.99        46\n",
      "\n",
      "    accuracy                           0.98      1009\n",
      "   macro avg       0.98      0.98      0.98      1009\n",
      "weighted avg       0.98      0.98      0.98      1009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_train_predict = clf.predict(x_train)\n",
    "print(\"For train data accuracy: \", accuracy_score(y_train, clf_train_predict)) \n",
    "\n",
    "clf_CV_predict = clf.predict(x_CV)\n",
    "print(\"For Cross-validation:\", accuracy_score(y_CV, clf_CV_predict))\n",
    "\n",
    "clf_test_predict = clf.predict(x_test)\n",
    "print(\"For test data:\", accuracy_score(y_test, clf_test_predict)) #very good i think :)\n",
    "print(confusion_matrix(y_test, clf_test_predict))\n",
    "print(classification_report(y_test, clf_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_clf = DecisionTreeClassifier(criterion='entropy', random_state=22, max_depth = 100, min_samples_leaf = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=100, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=22, splitter='best')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data accuracy:  0.972041827187672\n",
      "For Cross-validation: 0.7712871287128713\n",
      "For test data: 0.7869177403369673\n",
      "[[30  2  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 40  0  1  2  1  0  0  0  4  0  0  0  0  1  0  0  0  1  0  1  0  0  0]\n",
      " [ 0  1 33  3  0  0  0  1  1  1  0  4  0  0  1  0  1  1  0  2  0  0  0  0]\n",
      " [ 2  1  1 31  0  0  0  0  1  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0 25  3  0  2  0  0  0  0  0  1  0  0  0  0  0  1  1  1  0  0]\n",
      " [ 0  1  0  0  2 42  0  2  0  3  0  2  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 45  0  0  0  0  1  1  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  2  0 48  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0 38  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  1  0  1  0  0  0  0 23  0  0  0  0  0  2  0  1  1  1  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0  3 34  0  0  0  1  0  0  2  0  0  0  0  0  2]\n",
      " [ 0  0  1  0  1  0  1  0  2  0  0 30  0  0  0  0  0  0  0  0  0  1  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 41  2  0  0  0  1  0  1  0  1  0  1]\n",
      " [ 0  1  0  0  0  0  0  1  0  0  1  0  0 30  2  0  0  0  1  3  0  0  0  0]\n",
      " [ 0  2  0  0  0  1  1  0  1  0  0  0  0  4 18  0  0  0  0  0  1  0  0  0]\n",
      " [ 2  0  0  0  0  1  0  0  0  1  0  0  0  0  0 43  0  0  0  0  1  0  1  0]\n",
      " [ 0  0  1  3  0  0  0  0  0  0  0  0  0  0  0  0 35  0  0  3  0  1  0  0]\n",
      " [ 0  1  1  1  0  2  0  0  1  1  1  3  1  0  0  0  1 34  2  2  0  1  0  1]\n",
      " [ 0  2  0  1  0  0  1  0  0  1  0  0  0  1  2  0  0  1 26  2  1  0  0  0]\n",
      " [ 0  4  3  0  0  2  0  0  1  1  0  0  1  1  0  0  1  1  0 20  1  1  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  1  0  0  0  3  0  1  1  0  1  3 29  0  0  0]\n",
      " [ 0  2  1  0  0  0  1  0  0  0  3  0  0  0  0  0  1  1  0  1  0 28  0  0]\n",
      " [ 0  0  0  0  0  1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0 35  0]\n",
      " [ 0  1  1  0  2  0  1  0  0  0  0  0  0  0  0  0  0  0  1  1  3  0  0 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        35\n",
      "           1       0.66      0.78      0.71        51\n",
      "           2       0.75      0.67      0.71        49\n",
      "           3       0.76      0.82      0.78        38\n",
      "           4       0.74      0.71      0.72        35\n",
      "           5       0.75      0.79      0.77        53\n",
      "           6       0.88      0.92      0.90        49\n",
      "           7       0.89      0.94      0.91        51\n",
      "           8       0.84      0.95      0.89        40\n",
      "           9       0.57      0.74      0.65        31\n",
      "          10       0.85      0.77      0.81        44\n",
      "          11       0.71      0.81      0.76        37\n",
      "          12       0.93      0.87      0.90        47\n",
      "          13       0.68      0.77      0.72        39\n",
      "          14       0.72      0.64      0.68        28\n",
      "          15       0.91      0.88      0.90        49\n",
      "          16       0.88      0.81      0.84        43\n",
      "          17       0.81      0.64      0.72        53\n",
      "          18       0.79      0.68      0.73        38\n",
      "          19       0.50      0.54      0.52        37\n",
      "          20       0.76      0.72      0.74        40\n",
      "          21       0.78      0.74      0.76        38\n",
      "          22       0.95      0.92      0.93        38\n",
      "          23       0.90      0.78      0.84        46\n",
      "\n",
      "    accuracy                           0.79      1009\n",
      "   macro avg       0.79      0.78      0.78      1009\n",
      "weighted avg       0.79      0.79      0.79      1009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dec_train_predict = dec_clf.predict(x_train)\n",
    "print(\"For train data accuracy: \", accuracy_score(y_train, dec_train_predict)) \n",
    "\n",
    "dec_CV_predict = dec_clf.predict(x_CV)\n",
    "print(\"For Cross-validation:\", accuracy_score(y_CV, dec_CV_predict))\n",
    "\n",
    "dec_test_predict = dec_clf.predict(x_test)\n",
    "print(\"For test data:\", accuracy_score(y_test, dec_test_predict)) #pretty good i think :)\n",
    "print(confusion_matrix(y_test, dec_test_predict))\n",
    "print(classification_report(y_test, dec_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
