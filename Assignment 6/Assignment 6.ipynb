{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>News_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Naviforce Sport Saat 2016 ilə zövqlərin ahəngi</td>\n",
       "      <td>Naviforce Sport Saat 2016 ilə zövqlərin ahəngi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Sinir ,oynaq , sinir bel ağrılarına 3 gündə son !</td>\n",
       "      <td>Sinir ,oynaq , sinir bel ağrılarına 3 gündə so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Dəyərindən qat-qat aşağı qiymətə Mənzil</td>\n",
       "      <td>Dəyərindən qat-qat aşağı qiymətə MənzilDəyərin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>İdman</td>\n",
       "      <td>2024 və 2028-ci il olimpiadalarının keçiriləcə...</td>\n",
       "      <td>2024 və 2028-ci il olimpiadalarının keçiriləcə...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Türkiyədə zəlzələ</td>\n",
       "      <td>Türkiyədə zəlzələ  Türkiyədə daha bir zəlzələ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                              Title  \\\n",
       "0  Maraqlı     Naviforce Sport Saat 2016 ilə zövqlərin ahəngi   \n",
       "1  Maraqlı  Sinir ,oynaq , sinir bel ağrılarına 3 gündə son !   \n",
       "2  Maraqlı            Dəyərindən qat-qat aşağı qiymətə Mənzil   \n",
       "3    İdman  2024 və 2028-ci il olimpiadalarının keçiriləcə...   \n",
       "4    Dünya                                 Türkiyədə zəlzələ    \n",
       "\n",
       "                                        News_Article  \n",
       "0  Naviforce Sport Saat 2016 ilə zövqlərin ahəngi...  \n",
       "1  Sinir ,oynaq , sinir bel ağrılarına 3 gündə so...  \n",
       "2  Dəyərindən qat-qat aşağı qiymətə MənzilDəyərin...  \n",
       "3  2024 və 2028-ci il olimpiadalarının keçiriləcə...  \n",
       "4  Türkiyədə zəlzələ  Türkiyədə daha bir zəlzələ ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting data and preprocessing it\n",
    "data = pd.read_excel(\"azeri_news.xlsx\")\n",
    "data[\"News_Article\"] = data[\"Title\"] + data[\"News_Article\"] #I decided to concatenate both columns, cause we can use information\n",
    "                                                            #from the title and article and they will be equally useful \n",
    "                                                            #in training our model(maybe title will be more useful)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data(10% test, 90% train)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['News_Article'], data['Category'], test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#Encoding labels\n",
    "Encoder = LabelEncoder()\n",
    "Y_train = Encoder.fit_transform(Y_train)\n",
    "Y_test = Encoder.fit_transform(Y_test)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1000)\n",
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_features = 1000)\n",
    "count_vect.fit(data['News_Article'])\n",
    "X_train_counts = count_vect.transform(X_train)\n",
    "print(X_train_counts.shape)\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1000)\n",
      "(5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_features = 1000)\n",
    "tfidf_vect.fit(data['News_Article'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "print(X_train_tfidf.shape)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "print(X_test_tfidf.shape)\n",
    "#X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  80.47999999999999\n",
      "For test data: 0.8048\n",
      "[[1087  186    4   24   22   16]\n",
      " [ 199 1218   18   46   54   55]\n",
      " [  12   33   97    6    0    6]\n",
      " [  54   39    6  485    4   26]\n",
      " [  13   32    2    6  581    1]\n",
      " [  25   59    6   21    1  556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80      1339\n",
      "           1       0.78      0.77      0.77      1590\n",
      "           2       0.73      0.63      0.68       154\n",
      "           3       0.82      0.79      0.81       614\n",
      "           4       0.88      0.91      0.90       635\n",
      "           5       0.84      0.83      0.84       668\n",
      "\n",
      "    accuracy                           0.80      5000\n",
      "   macro avg       0.81      0.79      0.80      5000\n",
      "weighted avg       0.80      0.80      0.80      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "classifier_counts = LinearSVC(max_iter = 2000)\n",
    "classifier_counts.fit(X_train_counts, Y_train)\n",
    "predictions_SVM_counts = classifier_counts.predict(X_test_counts)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_counts, Y_test)*100)\n",
    "print(\"For test data:\", accuracy_score(Y_test, predictions_SVM_counts))\n",
    "print(confusion_matrix(Y_test, predictions_SVM_counts))\n",
    "print(classification_report(Y_test, predictions_SVM_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  81.3\n",
      "For test data: 0.813\n",
      "[[1097  169    3   29   22   19]\n",
      " [ 202 1220    8   49   56   55]\n",
      " [  12   40   88   11    0    3]\n",
      " [  46   30    5  506    5   22]\n",
      " [  11   25    2    6  589    2]\n",
      " [  20   55    3   25    0  565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80      1339\n",
      "           1       0.79      0.77      0.78      1590\n",
      "           2       0.81      0.57      0.67       154\n",
      "           3       0.81      0.82      0.82       614\n",
      "           4       0.88      0.93      0.90       635\n",
      "           5       0.85      0.85      0.85       668\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.82      0.79      0.80      5000\n",
      "weighted avg       0.81      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = LinearSVC(max_iter = 2000)\n",
    "classifier_tfidf.fit(X_train_tfidf, Y_train)\n",
    "predictions_SVM_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_tfidf, Y_test)*100)\n",
    "print(\"For test data:\", accuracy_score(Y_test, predictions_SVM_tfidf)) \n",
    "print(confusion_matrix(Y_test, predictions_SVM_tfidf))\n",
    "print(classification_report(Y_test, predictions_SVM_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(30,30,30), max_iter=200, alpha=0.0001,\n",
    "                     solver='adam', verbose=True,  random_state=22,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88493550\n",
      "Iteration 2, loss = 0.54891348\n",
      "Iteration 3, loss = 0.48867748\n",
      "Iteration 4, loss = 0.45367481\n",
      "Iteration 5, loss = 0.42726627\n",
      "Iteration 6, loss = 0.40281871\n",
      "Iteration 7, loss = 0.38352201\n",
      "Iteration 8, loss = 0.36496659\n",
      "Iteration 9, loss = 0.34795978\n",
      "Iteration 10, loss = 0.33324766\n",
      "Iteration 11, loss = 0.31935038\n",
      "Iteration 12, loss = 0.30530593\n",
      "Iteration 13, loss = 0.29091647\n",
      "Iteration 14, loss = 0.28004116\n",
      "Iteration 15, loss = 0.26690954\n",
      "Iteration 16, loss = 0.25938958\n",
      "Iteration 17, loss = 0.24812894\n",
      "Iteration 18, loss = 0.23738909\n",
      "Iteration 19, loss = 0.22801208\n",
      "Iteration 20, loss = 0.21908259\n",
      "Iteration 21, loss = 0.21214904\n",
      "Iteration 22, loss = 0.20417778\n",
      "Iteration 23, loss = 0.19854043\n",
      "Iteration 24, loss = 0.18830037\n",
      "Iteration 25, loss = 0.18434766\n",
      "Iteration 26, loss = 0.17468449\n",
      "Iteration 27, loss = 0.17117454\n",
      "Iteration 28, loss = 0.17054192\n",
      "Iteration 29, loss = 0.16348558\n",
      "Iteration 30, loss = 0.15467849\n",
      "Iteration 31, loss = 0.14767959\n",
      "Iteration 32, loss = 0.14530396\n",
      "Iteration 33, loss = 0.14414640\n",
      "Iteration 34, loss = 0.13811388\n",
      "Iteration 35, loss = 0.13782072\n",
      "Iteration 36, loss = 0.12940378\n",
      "Iteration 37, loss = 0.12387578\n",
      "Iteration 38, loss = 0.12416302\n",
      "Iteration 39, loss = 0.12053369\n",
      "Iteration 40, loss = 0.11992972\n",
      "Iteration 41, loss = 0.11567921\n",
      "Iteration 42, loss = 0.11421725\n",
      "Iteration 43, loss = 0.11282273\n",
      "Iteration 44, loss = 0.10700285\n",
      "Iteration 45, loss = 0.10640966\n",
      "Iteration 46, loss = 0.10762819\n",
      "Iteration 47, loss = 0.10779420\n",
      "Iteration 48, loss = 0.09882818\n",
      "Iteration 49, loss = 0.09686610\n",
      "Iteration 50, loss = 0.09831842\n",
      "Iteration 51, loss = 0.09264306\n",
      "Iteration 52, loss = 0.09610060\n",
      "Iteration 53, loss = 0.09625718\n",
      "Iteration 54, loss = 0.09136816\n",
      "Iteration 55, loss = 0.08741171\n",
      "Iteration 56, loss = 0.08664311\n",
      "Iteration 57, loss = 0.08351725\n",
      "Iteration 58, loss = 0.08511081\n",
      "Iteration 59, loss = 0.09110293\n",
      "Iteration 60, loss = 0.08236547\n",
      "Iteration 61, loss = 0.08534198\n",
      "Iteration 62, loss = 0.08128713\n",
      "Iteration 63, loss = 0.07892628\n",
      "Iteration 64, loss = 0.08104166\n",
      "Iteration 65, loss = 0.07896764\n",
      "Iteration 66, loss = 0.08252740\n",
      "Iteration 67, loss = 0.08214870\n",
      "Iteration 68, loss = 0.07472118\n",
      "Iteration 69, loss = 0.07457768\n",
      "Iteration 70, loss = 0.07554201\n",
      "Iteration 71, loss = 0.07650445\n",
      "Iteration 72, loss = 0.07161575\n",
      "Iteration 73, loss = 0.06988396\n",
      "Iteration 74, loss = 0.07157869\n",
      "Iteration 75, loss = 0.06928368\n",
      "Iteration 76, loss = 0.07373017\n",
      "Iteration 77, loss = 0.07220062\n",
      "Iteration 78, loss = 0.07661946\n",
      "Iteration 79, loss = 0.06695137\n",
      "Iteration 80, loss = 0.06766002\n",
      "Iteration 81, loss = 0.06901826\n",
      "Iteration 82, loss = 0.06550699\n",
      "Iteration 83, loss = 0.06664163\n",
      "Iteration 84, loss = 0.06473518\n",
      "Iteration 85, loss = 0.06439960\n",
      "Iteration 86, loss = 0.06735208\n",
      "Iteration 87, loss = 0.06990691\n",
      "Iteration 88, loss = 0.06446724\n",
      "Iteration 89, loss = 0.06520636\n",
      "Iteration 90, loss = 0.06328509\n",
      "Iteration 91, loss = 0.06325265\n",
      "Iteration 92, loss = 0.05802731\n",
      "Iteration 93, loss = 0.05929325\n",
      "Iteration 94, loss = 0.05699255\n",
      "Iteration 95, loss = 0.06326159\n",
      "Iteration 96, loss = 0.06131446\n",
      "Iteration 97, loss = 0.05809020\n",
      "Iteration 98, loss = 0.06448555\n",
      "Iteration 99, loss = 0.05981918\n",
      "Iteration 100, loss = 0.06003403\n",
      "Iteration 101, loss = 0.06106588\n",
      "Iteration 102, loss = 0.06159113\n",
      "Iteration 103, loss = 0.05819697\n",
      "Iteration 104, loss = 0.05725013\n",
      "Iteration 105, loss = 0.05756923\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30, 30, 30), random_state=22, tol=1e-09,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_counts, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data accuracy:  0.9832888888888889\n",
      "For test data: 0.7726\n",
      "[[1019  216    7   50   28   19]\n",
      " [ 224 1172   35   65   55   39]\n",
      " [  13   28   95   13    2    3]\n",
      " [  40   42    8  500    5   19]\n",
      " [  23   49    4    7  548    4]\n",
      " [  24   66    6   41    2  529]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      1339\n",
      "           1       0.75      0.74      0.74      1590\n",
      "           2       0.61      0.62      0.61       154\n",
      "           3       0.74      0.81      0.78       614\n",
      "           4       0.86      0.86      0.86       635\n",
      "           5       0.86      0.79      0.83       668\n",
      "\n",
      "    accuracy                           0.77      5000\n",
      "   macro avg       0.76      0.76      0.76      5000\n",
      "weighted avg       0.77      0.77      0.77      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_train_predict = clf.predict(X_train_counts)\n",
    "print(\"For train data accuracy: \", accuracy_score(Y_train, clf_train_predict)) \n",
    "\n",
    "clf_test_predict = clf.predict(X_test_counts)\n",
    "print(\"For test data:\", accuracy_score(Y_test, clf_test_predict))\n",
    "print(confusion_matrix(Y_test, clf_test_predict))\n",
    "print(classification_report(Y_test, clf_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07966302\n",
      "Iteration 2, loss = 0.57236999\n",
      "Iteration 3, loss = 0.51245798\n",
      "Iteration 4, loss = 0.48778357\n",
      "Iteration 5, loss = 0.47178633\n",
      "Iteration 6, loss = 0.45865035\n",
      "Iteration 7, loss = 0.44640049\n",
      "Iteration 8, loss = 0.43551139\n",
      "Iteration 9, loss = 0.42688572\n",
      "Iteration 10, loss = 0.41705751\n",
      "Iteration 11, loss = 0.40804892\n",
      "Iteration 12, loss = 0.40006795\n",
      "Iteration 13, loss = 0.39221361\n",
      "Iteration 14, loss = 0.38504290\n",
      "Iteration 15, loss = 0.37743772\n",
      "Iteration 16, loss = 0.37134917\n",
      "Iteration 17, loss = 0.36363814\n",
      "Iteration 18, loss = 0.35719614\n",
      "Iteration 19, loss = 0.35050869\n",
      "Iteration 20, loss = 0.34310440\n",
      "Iteration 21, loss = 0.33745303\n",
      "Iteration 22, loss = 0.33049847\n",
      "Iteration 23, loss = 0.32493394\n",
      "Iteration 24, loss = 0.31812837\n",
      "Iteration 25, loss = 0.31295215\n",
      "Iteration 26, loss = 0.30463468\n",
      "Iteration 27, loss = 0.29928226\n",
      "Iteration 28, loss = 0.29460489\n",
      "Iteration 29, loss = 0.28687591\n",
      "Iteration 30, loss = 0.28031155\n",
      "Iteration 31, loss = 0.27544130\n",
      "Iteration 32, loss = 0.26850303\n",
      "Iteration 33, loss = 0.26230200\n",
      "Iteration 34, loss = 0.25592673\n",
      "Iteration 35, loss = 0.25065584\n",
      "Iteration 36, loss = 0.24557081\n",
      "Iteration 37, loss = 0.24058809\n",
      "Iteration 38, loss = 0.23459171\n",
      "Iteration 39, loss = 0.22825163\n",
      "Iteration 40, loss = 0.22328859\n",
      "Iteration 41, loss = 0.21968709\n",
      "Iteration 42, loss = 0.21411073\n",
      "Iteration 43, loss = 0.20934703\n",
      "Iteration 44, loss = 0.20302059\n",
      "Iteration 45, loss = 0.20104877\n",
      "Iteration 46, loss = 0.19458818\n",
      "Iteration 47, loss = 0.18897672\n",
      "Iteration 48, loss = 0.18559475\n",
      "Iteration 49, loss = 0.18195680\n",
      "Iteration 50, loss = 0.17909628\n",
      "Iteration 51, loss = 0.17333918\n",
      "Iteration 52, loss = 0.16930884\n",
      "Iteration 53, loss = 0.16502021\n",
      "Iteration 54, loss = 0.16136142\n",
      "Iteration 55, loss = 0.15961333\n",
      "Iteration 56, loss = 0.15561595\n",
      "Iteration 57, loss = 0.15115127\n",
      "Iteration 58, loss = 0.14860015\n",
      "Iteration 59, loss = 0.14636173\n",
      "Iteration 60, loss = 0.14055197\n",
      "Iteration 61, loss = 0.13794978\n",
      "Iteration 62, loss = 0.13669935\n",
      "Iteration 63, loss = 0.13142509\n",
      "Iteration 64, loss = 0.12936933\n",
      "Iteration 65, loss = 0.12652843\n",
      "Iteration 66, loss = 0.12564043\n",
      "Iteration 67, loss = 0.12550185\n",
      "Iteration 68, loss = 0.12067258\n",
      "Iteration 69, loss = 0.11678262\n",
      "Iteration 70, loss = 0.11582313\n",
      "Iteration 71, loss = 0.11456314\n",
      "Iteration 72, loss = 0.11080637\n",
      "Iteration 73, loss = 0.10913518\n",
      "Iteration 74, loss = 0.10662766\n",
      "Iteration 75, loss = 0.10497102\n",
      "Iteration 76, loss = 0.10302809\n",
      "Iteration 77, loss = 0.10215728\n",
      "Iteration 78, loss = 0.10435373\n",
      "Iteration 79, loss = 0.10108791\n",
      "Iteration 80, loss = 0.09890648\n",
      "Iteration 81, loss = 0.09690409\n",
      "Iteration 82, loss = 0.09362051\n",
      "Iteration 83, loss = 0.09392529\n",
      "Iteration 84, loss = 0.09190770\n",
      "Iteration 85, loss = 0.09391739\n",
      "Iteration 86, loss = 0.08969019\n",
      "Iteration 87, loss = 0.08915645\n",
      "Iteration 88, loss = 0.09068526\n",
      "Iteration 89, loss = 0.08893059\n",
      "Iteration 90, loss = 0.08733659\n",
      "Iteration 91, loss = 0.08414975\n",
      "Iteration 92, loss = 0.08231112\n",
      "Iteration 93, loss = 0.08190089\n",
      "Iteration 94, loss = 0.08382261\n",
      "Iteration 95, loss = 0.08202631\n",
      "Iteration 96, loss = 0.08220255\n",
      "Iteration 97, loss = 0.08077039\n",
      "Iteration 98, loss = 0.08004142\n",
      "Iteration 99, loss = 0.07765998\n",
      "Iteration 100, loss = 0.07774427\n",
      "Iteration 101, loss = 0.07887745\n",
      "Iteration 102, loss = 0.07782475\n",
      "Iteration 103, loss = 0.07874289\n",
      "Iteration 104, loss = 0.07657537\n",
      "Iteration 105, loss = 0.07512925\n",
      "Iteration 106, loss = 0.07315264\n",
      "Iteration 107, loss = 0.07483874\n",
      "Iteration 108, loss = 0.07522694\n",
      "Iteration 109, loss = 0.07399539\n",
      "Iteration 110, loss = 0.07468390\n",
      "Iteration 111, loss = 0.07094691\n",
      "Iteration 112, loss = 0.07399225\n",
      "Iteration 113, loss = 0.07172775\n",
      "Iteration 114, loss = 0.07223118\n",
      "Iteration 115, loss = 0.06925557\n",
      "Iteration 116, loss = 0.07017553\n",
      "Iteration 117, loss = 0.06919338\n",
      "Iteration 118, loss = 0.07002152\n",
      "Iteration 119, loss = 0.06871884\n",
      "Iteration 120, loss = 0.06797254\n",
      "Iteration 121, loss = 0.06775496\n",
      "Iteration 122, loss = 0.06826439\n",
      "Iteration 123, loss = 0.06723480\n",
      "Iteration 124, loss = 0.06620329\n",
      "Iteration 125, loss = 0.06632721\n",
      "Iteration 126, loss = 0.06779021\n",
      "Iteration 127, loss = 0.06637228\n",
      "Iteration 128, loss = 0.06373481\n",
      "Iteration 129, loss = 0.06852211\n",
      "Iteration 130, loss = 0.06544697\n",
      "Iteration 131, loss = 0.06687688\n",
      "Iteration 132, loss = 0.06408577\n",
      "Iteration 133, loss = 0.06312858\n",
      "Iteration 134, loss = 0.06246609\n",
      "Iteration 135, loss = 0.06641300\n",
      "Iteration 136, loss = 0.06263177\n",
      "Iteration 137, loss = 0.06387532\n",
      "Iteration 138, loss = 0.06336521\n",
      "Iteration 139, loss = 0.06128046\n",
      "Iteration 140, loss = 0.05996488\n",
      "Iteration 141, loss = 0.06179157\n",
      "Iteration 142, loss = 0.06353853\n",
      "Iteration 143, loss = 0.06172819\n",
      "Iteration 144, loss = 0.06032174\n",
      "Iteration 145, loss = 0.06154648\n",
      "Iteration 146, loss = 0.06193692\n",
      "Iteration 147, loss = 0.06043943\n",
      "Iteration 148, loss = 0.06084370\n",
      "Iteration 149, loss = 0.06010161\n",
      "Iteration 150, loss = 0.05959616\n",
      "Iteration 151, loss = 0.06005736\n",
      "Iteration 152, loss = 0.05863924\n",
      "Iteration 153, loss = 0.05794751\n",
      "Iteration 154, loss = 0.05946115\n",
      "Iteration 155, loss = 0.05966637\n",
      "Iteration 156, loss = 0.05614805\n",
      "Iteration 157, loss = 0.05953356\n",
      "Iteration 158, loss = 0.05872439\n",
      "Iteration 159, loss = 0.05684811\n",
      "Iteration 160, loss = 0.05860633\n",
      "Iteration 161, loss = 0.05794012\n",
      "Iteration 162, loss = 0.05905019\n",
      "Iteration 163, loss = 0.05859325\n",
      "Iteration 164, loss = 0.05583141\n",
      "Iteration 165, loss = 0.05538928\n",
      "Iteration 166, loss = 0.05576161\n",
      "Iteration 167, loss = 0.05584757\n",
      "Iteration 168, loss = 0.05698785\n",
      "Iteration 169, loss = 0.05554245\n",
      "Iteration 170, loss = 0.05622985\n",
      "Iteration 171, loss = 0.05695531\n",
      "Iteration 172, loss = 0.05586820\n",
      "Iteration 173, loss = 0.05579170\n",
      "Iteration 174, loss = 0.05325868\n",
      "Iteration 175, loss = 0.05388898\n",
      "Iteration 176, loss = 0.05602151\n",
      "Iteration 177, loss = 0.05506291\n",
      "Iteration 178, loss = 0.05565355\n",
      "Iteration 179, loss = 0.05339117\n",
      "Iteration 180, loss = 0.05559918\n",
      "Iteration 181, loss = 0.05430475\n",
      "Iteration 182, loss = 0.05503591\n",
      "Iteration 183, loss = 0.05254128\n",
      "Iteration 184, loss = 0.05576315\n",
      "Iteration 185, loss = 0.05431320\n",
      "Iteration 186, loss = 0.05411903\n",
      "Iteration 187, loss = 0.05307379\n",
      "Iteration 188, loss = 0.05202300\n",
      "Iteration 189, loss = 0.05268449\n",
      "Iteration 190, loss = 0.05253480\n",
      "Iteration 191, loss = 0.05333816\n",
      "Iteration 192, loss = 0.05259346\n",
      "Iteration 193, loss = 0.05172953\n",
      "Iteration 194, loss = 0.05117501\n",
      "Iteration 195, loss = 0.05287060\n",
      "Iteration 196, loss = 0.05269528\n",
      "Iteration 197, loss = 0.05265996\n",
      "Iteration 198, loss = 0.05342840\n",
      "Iteration 199, loss = 0.05175805\n",
      "Iteration 200, loss = 0.05275991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(30, 30, 30), random_state=22, tol=1e-09,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data accuracy:  0.9862\n",
      "For test data: 0.7744\n",
      "[[1003  230    5   47   31   23]\n",
      " [ 196 1194   27   56   66   51]\n",
      " [  11   32   97    7    1    6]\n",
      " [  42   50    9  482    5   26]\n",
      " [  19   51    2    7  553    3]\n",
      " [  21   67    5   29    3  543]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.76      1339\n",
      "           1       0.74      0.75      0.74      1590\n",
      "           2       0.67      0.63      0.65       154\n",
      "           3       0.77      0.79      0.78       614\n",
      "           4       0.84      0.87      0.85       635\n",
      "           5       0.83      0.81      0.82       668\n",
      "\n",
      "    accuracy                           0.77      5000\n",
      "   macro avg       0.77      0.77      0.77      5000\n",
      "weighted avg       0.77      0.77      0.77      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_train_predict = clf.predict(X_train_tfidf)\n",
    "print(\"For train data accuracy: \", accuracy_score(Y_train, clf_train_predict)) \n",
    "\n",
    "clf_test_predict = clf.predict(X_test_tfidf)\n",
    "print(\"For test data:\", accuracy_score(Y_test, clf_test_predict))\n",
    "print(confusion_matrix(Y_test, clf_test_predict))\n",
    "print(classification_report(Y_test, clf_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
