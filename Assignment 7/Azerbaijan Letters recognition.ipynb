{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efficient-colony",
   "metadata": {},
   "source": [
    "# Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-brave",
   "metadata": {},
   "source": [
    "Here I used default libraries for data manipulationa and mathematical operation(pandas and numpy, correspondingly). Afterwards I set up Tensorflow backend and imported keras for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gorgeous-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-smoke",
   "metadata": {},
   "source": [
    "# Data with 32x32 shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-lunch",
   "metadata": {},
   "source": [
    "At first, I tried to make deep learning with 32x32 images, because you do not need any reshaping. <br>\n",
    "Below is load_data function for this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legendary-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pd.read_csv(filename, delimiter = \"#\")\n",
    "    data['Data'] = data['Data'].apply(lambda x: list(x))\n",
    "    for i, j in enumerate(data['Data']):\n",
    "        test_list = np.array(list(map(int, j)))\n",
    "        data.at[i, 'Data'] = test_list\n",
    "        data.at[i, 'Data'] = np.asarray(data.at[i, 'Data'].reshape(data.at[i, 'SizeH'], data.at[i, 'SizeW'])).astype('float32')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "about-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatN</th>\n",
       "      <th>PatType</th>\n",
       "      <th>PatProb</th>\n",
       "      <th>SizeH</th>\n",
       "      <th>SizeW</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatN  PatType  PatProb  SizeH  SizeW  \\\n",
       "0     0        0       10     32     32   \n",
       "1     0        0       10     32     32   \n",
       "2     0        0       10     32     32   \n",
       "3     0        0       10     32     32   \n",
       "4     0        0        9     32     32   \n",
       "\n",
       "                                                Data  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMoElEQVR4nO3dX4wd5XnH8e9Td7HLn6q4BLI1Vp0gRypCiUErF4kqoqVNXRQJuACFi8gXKJuLIBUpvbCoVOgdrQoRV0hLseJUlIAKCFShJshqhCJVLgs1xsQpIcglri2bFCJoqxqDn16csbp298/xOTPnnN3n+5FWZ86cmZ1Ho/3tOzPvmXciM5G09v3SuAuQNBqGXSrCsEtFGHapCMMuFWHYpSJ+eZiVI2IH8DCwDvjrzHxgueUviPW5gYuG2aQ68LnP//dA67154MKWK9Gw/of/4qM8GYt9FoP2s0fEOuBN4A+AI8DLwJ2Z+aOl1vnV2Ji/HTcNtD1153tH9w+03h/+xrZW69Dw9uVePsj3Fg37MIfx24G3MvPtzPwI+C5wyxC/T1KHhgn7JuBnC94faeZJmkDDnLMvdqjw/84JImIWmAXYgOd40rgM07IfATYveH8lcPTchTJzLjNnMnNmivVDbE7SMIYJ+8vA1oj4TERcAHwFeL6dsiS1beDD+Mz8OCLuBr5Hr+ttd2a+0VplatWgV9y1dgzVz56ZLwAvtFSLpA75DTqpCMMuFWHYpSIMu1SEYZeKGOpqvNY+b3ZZO2zZpSIMu1SEYZeKMOxSEYZdKsKr8WuMN7xoKbbsUhGGXSrCsEtFGHapCMMuFWHYpSLseluF7F7TIGzZpSIMu1SEYZeKMOxSEYZdKsKwS0UM1fUWEYeBD4FPgI8zc6aNojQ5luvmc3y61aWNfvbfzcyft/B7JHXIw3ipiGHDnsD3I+KViJhtoyBJ3Rj2MP6GzDwaEZcDL0bEjzPzpYULNP8EZgE2cOGQm5M0qKFa9sw82ryeAJ4Fti+yzFxmzmTmzBTrh9mcpCEMHPaIuCgiLjkzDXwJONhWYZLaNcxh/BXAsxFx5vf8bWb+QytVSWrdwGHPzLeBL7RYi6QO2fUmFWHYpSIMu1SEYZeKMOxSEQ44qYF5R9zqYssuFWHYpSIMu1SEYZeKMOxSEYZdKsKwS0UYdqkIwy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUxIpj0EXEbuDLwInMvKaZtxF4EtgCHAbuyMz3uyuznuXGd1sNlqrfsenGp5+W/dvAjnPm7QL2ZuZWYG/zXtIEWzHszfPW3ztn9i3AnmZ6D3Bru2VJatug5+xXZOYxgOb18vZKktSFzseNj4hZYBZgAxd2vTlJSxi0ZT8eEdMAzeuJpRbMzLnMnMnMmSnWD7g5ScMaNOzPAzub6Z3Ac+2UI6kr/XS9PQHcCFwWEUeA+4AHgKci4i7gHeD2LotUt5brDlvtXYD6PyuGPTPvXOKjm1quRVKH/AadVIRhl4ow7FIRhl0qwrBLRXT+DTpNBu82ky27VIRhl4ow7FIRhl0qwrBLRRh2qQi73rSstu+IW24duwe7ZcsuFWHYpSIMu1SEYZeKMOxSEV6NHyPHdzubV+q7ZcsuFWHYpSIMu1SEYZeKMOxSEYZdKqKfxz/tBr4MnMjMa5p59wNfA95tFrs3M1/oqkj1zy4qLaWflv3bwI5F5n8rM7c1PwZdmnArhj0zXwLeG0Etkjo0zDn73RFxICJ2R8SlrVUkqRODhv0R4CpgG3AMeHCpBSNiNiLmI2L+FCcH3JykYQ0U9sw8npmfZOZp4FFg+zLLzmXmTGbOTLF+0DolDWmgsEfE9IK3twEH2ylHUlf66Xp7ArgRuCwijgD3ATdGxDYggcPA17srcXVby3e2tT0+3XK8I254K4Y9M+9cZPZjHdQiqUN+g04qwrBLRRh2qQjDLhVh2KUiHHByFbKrSYOwZZeKMOxSEYZdKsKwS0UYdqkIwy4VYdebOrFU92AXdwEO8jsrdl/asktFGHapCMMuFWHYpSIMu1SEV+NXoUkZj201j683aO2r+Sq+LbtUhGGXijDsUhGGXSrCsEtFGHapiH4e/7QZ+A7waeA0MJeZD0fERuBJYAu9R0DdkZnvd1fq5JqkLqhJqkWTpZ+W/WPgm5n5W8D1wDci4mpgF7A3M7cCe5v3kibUimHPzGOZ+Woz/SFwCNgE3ALsaRbbA9zaUY2SWnBe5+wRsQW4FtgHXJGZx6D3DwG4vPXqJLWm77BHxMXA08A9mfnBeaw3GxHzETF/ipOD1CipBX2FPSKm6AX98cx8ppl9PCKmm8+ngROLrZuZc5k5k5kzU6xvo2ZJA1gx7BER9J7HfigzH1rw0fPAzmZ6J/Bc++VJaks/d73dAHwVeD0i9jfz7gUeAJ6KiLuAd4DbO6lQGrHVfGfbclYMe2b+EIglPr6p3XIkdcVv0ElFGHapCMMuFWHYpSIMu1SEA06uQmu1a2g5bd/NV3Ef2rJLRRh2qQjDLhVh2KUiDLtUhGGXirDrbUJV7BoapUl5Xt4o2bJLRRh2qQjDLhVh2KUiDLtUhFfjz4OPVtJqZssuFWHYpSIMu1SEYZeKMOxSEYZdKmLFrreI2Ax8B/g0cBqYy8yHI+J+4GvAu82i92bmC10VqtqWuznFLtH+9NPP/jHwzcx8NSIuAV6JiBebz76VmX/VXXmS2tLPs96OAcea6Q8j4hCwqevCJLXrvM7ZI2ILcC2wr5l1d0QciIjdEXFp28VJak/fYY+Ii4GngXsy8wPgEeAqYBu9lv/BJdabjYj5iJg/xcnhK5Y0kL7CHhFT9IL+eGY+A5CZxzPzk8w8DTwKbF9s3cycy8yZzJyZYn1bdUs6TyuGPSICeAw4lJkPLZg/vWCx24CD7ZcnqS39XI2/Afgq8HpE7G/m3QvcGRHbgAQOA1/voD5p5Nbq+HT9XI3/IRCLfGSfurSK+A06qQjDLhVh2KUiDLtUhGGXinDAyTFazd04k8Q74vpjyy4VYdilIgy7VIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUiDLtUhGGXijDsUhHe9XYO75LSWmXLLhVh2KUiDLtUhGGXijDsUhErXo2PiA3AS8D6Zvm/y8z7ImIj8CSwhd7jn+7IzPe7K3V1cpw5TYp+WvaTwO9l5hfoPZ55R0RcD+wC9mbmVmBv817ShFox7Nnzn83bqeYngVuAPc38PcCtXRQoqR39Pp99XfME1xPAi5m5D7giM48BNK+Xd1alpKH1FfbM/CQztwFXAtsj4pp+NxARsxExHxHzpzg5YJmShnVeV+Mz8xfAD4AdwPGImAZoXk8ssc5cZs5k5swU64erVtLAVgx7RHwqIn6tmf4V4PeBHwPPAzubxXYCz3VUo6QW9HMjzDSwJyLW0fvn8FRm/n1E/BPwVETcBbwD3N5hnZKGtGLYM/MAcO0i8/8DuKmLoiS1z2/QSUUYdqkIwy4VYdilIgy7VERk5ug2FvEu8G/N28uAn49s40uzjrNZx9lWWx2/mZmfWuyDkYb9rA1HzGfmzFg2bh3WUbAOD+OlIgy7VMQ4wz43xm0vZB1ns46zrZk6xnbOLmm0PIyXihhL2CNiR0T8a0S8FRFjG7suIg5HxOsRsT8i5ke43d0RcSIiDi6YtzEiXoyInzSvl46pjvsj4t+bfbI/Im4eQR2bI+IfI+JQRLwREX/czB/pPlmmjpHuk4jYEBH/HBGvNXX8eTN/uP2RmSP9AdYBPwU+C1wAvAZcPeo6mloOA5eNYbtfBK4DDi6Y95fArmZ6F/AXY6rjfuBPRrw/poHrmulLgDeBq0e9T5apY6T7BAjg4mZ6CtgHXD/s/hhHy74deCsz387Mj4Dv0hu8sozMfAl475zZIx/Ac4k6Ri4zj2Xmq830h8AhYBMj3ifL1DFS2dP6IK/jCPsm4GcL3h9hDDu0kcD3I+KViJgdUw1nTNIAnndHxIHmML/z04mFImILvfETxjqo6Tl1wIj3SReDvI4j7LHIvHF1CdyQmdcBfwR8IyK+OKY6JskjwFX0nhFwDHhwVBuOiIuBp4F7MvODUW23jzpGvk9yiEFelzKOsB8BNi94fyVwdAx1kJlHm9cTwLP0TjHGpa8BPLuWmcebP7TTwKOMaJ9ExBS9gD2emc80s0e+TxarY1z7pNn2LzjPQV6XMo6wvwxsjYjPRMQFwFfoDV45UhFxUURccmYa+BJwcPm1OjURA3ie+WNq3MYI9klEBPAYcCgzH1rw0Uj3yVJ1jHqfdDbI66iuMJ5ztfFmelc6fwr86Zhq+Cy9noDXgDdGWQfwBL3DwVP0jnTuAn6d3mO0ftK8bhxTHX8DvA4caP64pkdQx+/QO5U7AOxvfm4e9T5Zpo6R7hPg88C/NNs7CPxZM3+o/eE36KQi/AadVIRhl4ow7FIRhl0qwrBLRRh2qQjDLhVh2KUi/hfg6m6kcIN9UwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = load_data('HandChars32_150_Train.txt')\n",
    "print(data_train.shape)\n",
    "plt.imshow(data_train.loc[0, 'Data'])\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unusual-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatN</th>\n",
       "      <th>PatType</th>\n",
       "      <th>PatProb</th>\n",
       "      <th>SizeH</th>\n",
       "      <th>SizeW</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatN  PatType  PatProb  SizeH  SizeW  \\\n",
       "0     0        0       10     32     32   \n",
       "1     0        0       10     32     32   \n",
       "2     0        0       10     32     32   \n",
       "3     0        0       10     32     32   \n",
       "4     0        0       10     32     32   \n",
       "\n",
       "                                                Data  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test = load_data('HandChars32_50_Test.txt')\n",
    "print(data_test.shape)\n",
    "plt.imshow(data_test.loc[0, 'Data'])\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-deviation",
   "metadata": {},
   "source": [
    "# Reshaping of arrays for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-birth",
   "metadata": {},
   "source": [
    "Here I had a problem with shape after receiving data from dataframe, so constructed helper function for reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valued-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaper(arr):\n",
    "    out = np.array([])\n",
    "    for i in arr:\n",
    "        out = np.append(out, i)\n",
    "    return out.reshape(arr.shape[0], arr[0].shape[0], arr[0].shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complicated-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 28)\n"
     ]
    }
   ],
   "source": [
    "x_train = reshaper(data_train['Data'])\n",
    "x_test = reshaper(data_test['Data'])\n",
    "y_train = data_train['PatN']\n",
    "y_test = data_test['PatN']\n",
    "\n",
    "num_classes = len(data_train.loc[:,'PatN'].unique())\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "medium-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 32, 32, 1)\n",
      "(1400, 32, 32, 1)\n",
      "(4200, 28)\n",
      "(1400, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-revelation",
   "metadata": {},
   "source": [
    "Constructing our CNN model with  3 convolutional layer, 1 pooling and adding dropout for making our model more robust to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "constant-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        832       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                14364     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 132,508\n",
      "Trainable params: 132,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "inp_shape = (32, 32, 1)\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (1,1), padding='same',\n",
    "                 input_shape=inp_shape))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dying-straight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 2.1116 - accuracy: 0.3781 - val_loss: 1.5671 - val_accuracy: 0.5514\n",
      "Epoch 2/15\n",
      "132/132 [==============================] - 5s 35ms/step - loss: 0.9913 - accuracy: 0.6912 - val_loss: 1.2899 - val_accuracy: 0.6564\n",
      "Epoch 3/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.6846 - accuracy: 0.7843 - val_loss: 1.2052 - val_accuracy: 0.6929\n",
      "Epoch 4/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.5272 - accuracy: 0.8371 - val_loss: 0.9142 - val_accuracy: 0.7464\n",
      "Epoch 5/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.3991 - accuracy: 0.8714 - val_loss: 0.9169 - val_accuracy: 0.7686\n",
      "Epoch 6/15\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3305 - accuracy: 0.8969 - val_loss: 1.1132 - val_accuracy: 0.7400\n",
      "Epoch 7/15\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2636 - accuracy: 0.9162 - val_loss: 0.8888 - val_accuracy: 0.7950\n",
      "Epoch 8/15\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2358 - accuracy: 0.9210 - val_loss: 0.8642 - val_accuracy: 0.7907\n",
      "Epoch 9/15\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.1884 - accuracy: 0.9360 - val_loss: 0.8795 - val_accuracy: 0.8050\n",
      "Epoch 10/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.1912 - accuracy: 0.9362 - val_loss: 0.7974 - val_accuracy: 0.8100\n",
      "Epoch 11/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.1573 - accuracy: 0.9512 - val_loss: 0.9225 - val_accuracy: 0.8057\n",
      "Epoch 12/15\n",
      "132/132 [==============================] - 5s 35ms/step - loss: 0.1359 - accuracy: 0.9536 - val_loss: 0.8632 - val_accuracy: 0.8114\n",
      "Epoch 13/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.1292 - accuracy: 0.9590 - val_loss: 0.8088 - val_accuracy: 0.8271\n",
      "Epoch 14/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.0992 - accuracy: 0.9655 - val_loss: 0.8091 - val_accuracy: 0.8286\n",
      "Epoch 15/15\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.0994 - accuracy: 0.9669 - val_loss: 0.9460 - val_accuracy: 0.8179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b38a68948>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-accreditation",
   "metadata": {},
   "source": [
    "The layer for Fully connected Neural Network can take only one dimensional array, so I reshaped my data for feeding it to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loaded-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_fully = data_train['PatN']\n",
    "y_test_fully = data_test['PatN']\n",
    "x_train_fully = data_train['Data']\n",
    "x_test_fully = data_test['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "studied-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200,)\n",
      "(1400,)\n",
      "(4200,)\n",
      "(1400,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_fully.shape)\n",
    "print(x_test_fully.shape)\n",
    "print(y_train_fully.shape)\n",
    "print(y_test_fully.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "perceived-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf(arr):\n",
    "    out = np.array([])\n",
    "    for i in arr:\n",
    "        out = np.append(out, i)\n",
    "    return out.reshape(arr.shape[0], arr[0].shape[0] * arr[0].shape[1])\n",
    "x_train_fully = transf(data_train['Data'])\n",
    "x_test_fully = transf(data_test['Data'])\n",
    "y_train_fully = keras.utils.to_categorical(y_train_fully, num_classes)\n",
    "y_test_fully = keras.utils.to_categorical(y_test_fully, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cognitive-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_fully.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "upper-video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_fully.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "velvet-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 28)\n",
      "(1400, 28)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_fully.shape)\n",
    "print(y_test_fully.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-somalia",
   "metadata": {},
   "source": [
    "Fully connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "irish-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 3,177,500\n",
      "Trainable params: 3,177,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(1024, input_shape=(1024,), activation=\"relu\"))\n",
    "model_2.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2.add(Dense(28, activation=\"softmax\"))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "guilty-criticism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 3.2912 - accuracy: 0.0781 - val_loss: 3.2719 - val_accuracy: 0.0893\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 3.1664 - accuracy: 0.2221 - val_loss: 3.1779 - val_accuracy: 0.1421\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 3.0275 - accuracy: 0.3690 - val_loss: 3.0637 - val_accuracy: 0.1950\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 2.8522 - accuracy: 0.4881 - val_loss: 2.9250 - val_accuracy: 0.2336\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 2.6256 - accuracy: 0.5783 - val_loss: 2.7379 - val_accuracy: 0.3121\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 2.3452 - accuracy: 0.6619 - val_loss: 2.5197 - val_accuracy: 0.3614\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 2.0289 - accuracy: 0.7000 - val_loss: 2.3332 - val_accuracy: 0.3864\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 1.7171 - accuracy: 0.7331 - val_loss: 2.1131 - val_accuracy: 0.4279\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 1.4474 - accuracy: 0.7524 - val_loss: 1.9706 - val_accuracy: 0.4564\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 1.2323 - accuracy: 0.7702 - val_loss: 1.8876 - val_accuracy: 0.4607\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 1.0674 - accuracy: 0.7940 - val_loss: 1.7549 - val_accuracy: 0.5043\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.9412 - accuracy: 0.8079 - val_loss: 1.6737 - val_accuracy: 0.5214\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.8424 - accuracy: 0.8233 - val_loss: 1.6449 - val_accuracy: 0.5293\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.7667 - accuracy: 0.8293 - val_loss: 1.5995 - val_accuracy: 0.5364\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.7045 - accuracy: 0.8438 - val_loss: 1.6061 - val_accuracy: 0.5350\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.6531 - accuracy: 0.8469 - val_loss: 1.5735 - val_accuracy: 0.5557\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.6095 - accuracy: 0.8567 - val_loss: 1.5294 - val_accuracy: 0.5579\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.5715 - accuracy: 0.8652 - val_loss: 1.4819 - val_accuracy: 0.5729\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.5390 - accuracy: 0.8693 - val_loss: 1.5243 - val_accuracy: 0.5721\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.5081 - accuracy: 0.8745 - val_loss: 1.4749 - val_accuracy: 0.5964\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.4848 - accuracy: 0.8831 - val_loss: 1.4404 - val_accuracy: 0.5964\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.4584 - accuracy: 0.8893 - val_loss: 1.4650 - val_accuracy: 0.6050\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.4377 - accuracy: 0.8912 - val_loss: 1.4479 - val_accuracy: 0.6007\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.4168 - accuracy: 0.8950 - val_loss: 1.4442 - val_accuracy: 0.6093\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3994 - accuracy: 0.9038 - val_loss: 1.4379 - val_accuracy: 0.6100\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.3836 - accuracy: 0.9071 - val_loss: 1.4599 - val_accuracy: 0.6043\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3678 - accuracy: 0.9100 - val_loss: 1.4437 - val_accuracy: 0.6179\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.3518 - accuracy: 0.9150 - val_loss: 1.3954 - val_accuracy: 0.6307\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3391 - accuracy: 0.9186 - val_loss: 1.4312 - val_accuracy: 0.6200\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.3255 - accuracy: 0.9250 - val_loss: 1.4126 - val_accuracy: 0.6364\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3135 - accuracy: 0.9245 - val_loss: 1.4031 - val_accuracy: 0.6314\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3023 - accuracy: 0.9281 - val_loss: 1.4420 - val_accuracy: 0.6429\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.2907 - accuracy: 0.9333 - val_loss: 1.4288 - val_accuracy: 0.6357\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2798 - accuracy: 0.9371 - val_loss: 1.4162 - val_accuracy: 0.6421\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2717 - accuracy: 0.9393 - val_loss: 1.4302 - val_accuracy: 0.6429\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2617 - accuracy: 0.9402 - val_loss: 1.4037 - val_accuracy: 0.6500\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.2528 - accuracy: 0.9460 - val_loss: 1.4378 - val_accuracy: 0.6407\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2447 - accuracy: 0.9505 - val_loss: 1.4220 - val_accuracy: 0.6521\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2375 - accuracy: 0.9519 - val_loss: 1.4552 - val_accuracy: 0.6479\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2296 - accuracy: 0.9540 - val_loss: 1.4353 - val_accuracy: 0.6521\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2211 - accuracy: 0.9545 - val_loss: 1.4319 - val_accuracy: 0.6579\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2141 - accuracy: 0.9571 - val_loss: 1.4369 - val_accuracy: 0.6550\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2077 - accuracy: 0.9595 - val_loss: 1.4404 - val_accuracy: 0.6571\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2018 - accuracy: 0.9610 - val_loss: 1.4449 - val_accuracy: 0.6636\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1951 - accuracy: 0.9629 - val_loss: 1.4365 - val_accuracy: 0.6607\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1892 - accuracy: 0.9652 - val_loss: 1.4641 - val_accuracy: 0.6586\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1826 - accuracy: 0.9674 - val_loss: 1.4582 - val_accuracy: 0.6636\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1770 - accuracy: 0.9693 - val_loss: 1.4426 - val_accuracy: 0.6743\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 3s 21ms/step - loss: 0.1715 - accuracy: 0.9700 - val_loss: 1.4477 - val_accuracy: 0.6650\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1672 - accuracy: 0.9710 - val_loss: 1.4678 - val_accuracy: 0.6657\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(SGD(lr = .003), \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2.fit(x_train_fully, y_train_fully, validation_data=(x_test_fully, y_test_fully), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-click",
   "metadata": {},
   "source": [
    "As we see, with Convolutional layers, model have more validation_accuracy, than default fully connected neural net. Also CNN is faster in training than Fully connected, due to less number of parameters in model(Check model.summary() sections).\n",
    "Also the fully connected neural net with the same number of parameters will have less power in terms of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-stuff",
   "metadata": {},
   "source": [
    "# Data with Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-jason",
   "metadata": {},
   "source": [
    "Here I found that data are in (256, 1) shape, so i decided to reshape it to (16, 16, 1) for feeding to CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "numerical-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_feature(filename):\n",
    "    data = pd.read_csv(filename, delimiter = \"#\")\n",
    "    for i, j in enumerate(data['Data']):\n",
    "        j = j.replace(',', '.').split(';')\n",
    "        j = np.array(list(map(float, j)))\n",
    "        data.at[i, 'Data'] = j\n",
    "        data.at[i, 'Data'] = np.asarray(data.at[i, 'Data'].reshape(16, 16, 1)).astype('float32')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sharing-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feat = load_data_feature(\"HandChars_FeatExt_PDC4428_150_Train.txt\")\n",
    "test_data_feat = load_data_feature(\"HandChars_FeatExt_PDC4428_50_Test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cloudy-seller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "filled-outside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_feat.at[0, 'Data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "thick-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 16, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_feat = reshaper(train_data_feat['Data'])\n",
    "x_test_feat = reshaper(test_data_feat['Data'])\n",
    "y_train_feat = train_data_feat['PatN']\n",
    "y_test_feat = test_data_feat['PatN']\n",
    "print(x_train_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exotic-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 28)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(data_train.loc[:,'PatN'].unique())\n",
    "y_train_feat = keras.utils.to_categorical(y_train_feat, num_classes)\n",
    "y_test_feat = keras.utils.to_categorical(y_test_feat, num_classes)\n",
    "print(y_train_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-married",
   "metadata": {},
   "source": [
    "The same CNN as previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "italian-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        832       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 28)                14364     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 132,508\n",
      "Trainable params: 132,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_feat = Sequential()\n",
    "inp_shape = (16, 16, 1)\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1_feat.add(Conv2D(32, (5, 5), strides = (1,1), padding='same',\n",
    "                 input_shape=inp_shape))\n",
    "model_1_feat.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1_feat.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1_feat.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1_feat.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1_feat.add(Dropout(0.25))\n",
    "\n",
    "model_1_feat.add(Conv2D(32, (5, 5), strides = (2,2), padding = 'same'))\n",
    "model_1_feat.add(Activation('relu'))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1_feat.add(Flatten())\n",
    "model_1_feat.add(Dense(512))\n",
    "model_1_feat.add(Activation('relu'))\n",
    "model_1_feat.add(Dropout(0.5))\n",
    "model_1_feat.add(Dense(num_classes))\n",
    "model_1_feat.add(Activation('softmax'))\n",
    "\n",
    "model_1_feat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "white-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 2.2909 - accuracy: 0.3167 - val_loss: 1.6290 - val_accuracy: 0.4393\n",
      "Epoch 2/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.9738 - accuracy: 0.6786 - val_loss: 1.2300 - val_accuracy: 0.6021\n",
      "Epoch 3/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.6082 - accuracy: 0.7910 - val_loss: 1.0120 - val_accuracy: 0.7114\n",
      "Epoch 4/15\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.4531 - accuracy: 0.8467 - val_loss: 0.9124 - val_accuracy: 0.7279\n",
      "Epoch 5/15\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.3647 - accuracy: 0.8767 - val_loss: 0.9936 - val_accuracy: 0.7271\n",
      "Epoch 6/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.3154 - accuracy: 0.8888 - val_loss: 0.9225 - val_accuracy: 0.7429\n",
      "Epoch 7/15\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.2612 - accuracy: 0.9098 - val_loss: 0.7213 - val_accuracy: 0.8143\n",
      "Epoch 8/15\n",
      "132/132 [==============================] - 1s 11ms/step - loss: 0.2208 - accuracy: 0.9260 - val_loss: 0.8039 - val_accuracy: 0.8021\n",
      "Epoch 9/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.2152 - accuracy: 0.9267 - val_loss: 0.9155 - val_accuracy: 0.7879\n",
      "Epoch 10/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.1813 - accuracy: 0.9362 - val_loss: 0.8746 - val_accuracy: 0.8036\n",
      "Epoch 11/15\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.1576 - accuracy: 0.9498 - val_loss: 0.8801 - val_accuracy: 0.8093\n",
      "Epoch 12/15\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.1599 - accuracy: 0.9405 - val_loss: 0.8932 - val_accuracy: 0.8050\n",
      "Epoch 13/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.1428 - accuracy: 0.9481 - val_loss: 0.8356 - val_accuracy: 0.8243\n",
      "Epoch 14/15\n",
      "132/132 [==============================] - 2s 11ms/step - loss: 0.1179 - accuracy: 0.9581 - val_loss: 0.9765 - val_accuracy: 0.8179\n",
      "Epoch 15/15\n",
      "132/132 [==============================] - 2s 12ms/step - loss: 0.1046 - accuracy: 0.9669 - val_loss: 0.9623 - val_accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b444158c8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1_feat.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1_feat.fit(x_train_feat, y_train_feat,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test_feat, y_test_feat),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "statutory-annual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 16, 16, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "touched-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_feat_fully = x_train_feat.reshape(4200, 256)\n",
    "y_train_feat_fully = y_train_feat.reshape(4200, 28)\n",
    "x_test_feat_fully = x_test_feat.reshape(1400, 256)\n",
    "y_test_feat_fully = y_test_feat.reshape(1400, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "convertible-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 2,391,068\n",
      "Trainable params: 2,391,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2_feat = Sequential()\n",
    "model_2_feat.add(Dense(1024, input_shape=(256,), activation=\"relu\"))\n",
    "model_2_feat.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2_feat.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2_feat.add(Dense(28, activation=\"softmax\"))\n",
    "\n",
    "model_2_feat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "defensive-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 1.2067 - accuracy: 0.6740 - val_loss: 1.2534 - val_accuracy: 0.6364\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.3820 - accuracy: 0.8988 - val_loss: 0.9110 - val_accuracy: 0.7429\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.2465 - accuracy: 0.9350 - val_loss: 0.7931 - val_accuracy: 0.7607\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.1876 - accuracy: 0.9550 - val_loss: 0.8314 - val_accuracy: 0.7643\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.1475 - accuracy: 0.9671 - val_loss: 0.8069 - val_accuracy: 0.7757\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.1220 - accuracy: 0.9745 - val_loss: 0.7719 - val_accuracy: 0.7886\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.1027 - accuracy: 0.9798 - val_loss: 0.8017 - val_accuracy: 0.7871\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0877 - accuracy: 0.9838 - val_loss: 0.8164 - val_accuracy: 0.7814\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0783 - accuracy: 0.9862 - val_loss: 0.6868 - val_accuracy: 0.8093\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0688 - accuracy: 0.9881 - val_loss: 0.7398 - val_accuracy: 0.7993\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0608 - accuracy: 0.9914 - val_loss: 0.7403 - val_accuracy: 0.7979\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0539 - accuracy: 0.9943 - val_loss: 0.7454 - val_accuracy: 0.8014\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0480 - accuracy: 0.9948 - val_loss: 0.7738 - val_accuracy: 0.8029\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0447 - accuracy: 0.9955 - val_loss: 0.6862 - val_accuracy: 0.8086\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0405 - accuracy: 0.9971 - val_loss: 0.7806 - val_accuracy: 0.7979\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0369 - accuracy: 0.9976 - val_loss: 0.7360 - val_accuracy: 0.8064\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0351 - accuracy: 0.9974 - val_loss: 0.7372 - val_accuracy: 0.8079\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0316 - accuracy: 0.9986 - val_loss: 0.8466 - val_accuracy: 0.7914\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0306 - accuracy: 0.9986 - val_loss: 0.7474 - val_accuracy: 0.8093\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0282 - accuracy: 0.9981 - val_loss: 0.7192 - val_accuracy: 0.8164\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0264 - accuracy: 0.9979 - val_loss: 0.7624 - val_accuracy: 0.8071\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0248 - accuracy: 0.9988 - val_loss: 0.7492 - val_accuracy: 0.8121\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0236 - accuracy: 0.9988 - val_loss: 0.7903 - val_accuracy: 0.8007\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0222 - accuracy: 0.9993 - val_loss: 0.7474 - val_accuracy: 0.8107\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.8271\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0203 - accuracy: 0.9995 - val_loss: 0.7547 - val_accuracy: 0.8150\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.7538 - val_accuracy: 0.8107\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0182 - accuracy: 0.9993 - val_loss: 0.7892 - val_accuracy: 0.8093\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.7796 - val_accuracy: 0.8129\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.8171\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.7634 - val_accuracy: 0.8093\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.7706 - val_accuracy: 0.8164\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.8149 - val_accuracy: 0.8057\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.7700 - val_accuracy: 0.8179\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.7897 - val_accuracy: 0.8129\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.7602 - val_accuracy: 0.8186\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.7803 - val_accuracy: 0.8129\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.7792 - val_accuracy: 0.8171\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.7719 - val_accuracy: 0.8186\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.7811 - val_accuracy: 0.8171\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.7771 - val_accuracy: 0.8150\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.7690 - val_accuracy: 0.8221\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.7769 - val_accuracy: 0.8186\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.7759 - val_accuracy: 0.8164\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.7782 - val_accuracy: 0.8214\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 0.8150\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 2s 18ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 0.8164\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.7991 - val_accuracy: 0.8193\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 2s 16ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.7783 - val_accuracy: 0.8200\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 2s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.7704 - val_accuracy: 0.8236\n"
     ]
    }
   ],
   "source": [
    "model_2_feat.compile(SGD(lr = .003), \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2_feat.fit(x_train_feat_fully, y_train_feat_fully, validation_data=(x_test_feat_fully, y_test_feat_fully), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-research",
   "metadata": {},
   "source": [
    "As we see the neural nets accuracy was exactly the same. I think that it is the effect of feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-making",
   "metadata": {},
   "source": [
    "# Data with unequeal height and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "suffering-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_unequal(filename):\n",
    "    data = pd.read_csv(filename, delimiter = \"#\")\n",
    "    data['Data'] = data['Data'].apply(lambda x: list(x))\n",
    "    for i, j in enumerate(data['Data']):\n",
    "        test_list = np.array(list(map(int, j)))\n",
    "        data.at[i, 'Data'] = test_list\n",
    "        data.at[i, 'Data'] = np.asarray(data.at[i, 'Data'].reshape(data.at[i, 'SizeH'], data.at[i, 'SizeW'])).astype('float32')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rural-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uneq = load_data_unequal(\"HandChars_150_Train.txt\")\n",
    "test_uneq = load_data_unequal(\"HandChars_50_Test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "happy-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in enumerate(train_uneq['Data']):\n",
    "    img = Image.fromarray(j).resize(size=(32, 32))\n",
    "    train_uneq.at[i, 'Data'] = np.array(img).reshape(32, 32, 1)\n",
    "    \n",
    "for i, j in enumerate(test_uneq['Data']):\n",
    "    img = Image.fromarray(j).resize(size=(32, 32))\n",
    "    test_uneq.at[i, 'Data'] = np.array(img).reshape(32, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "partial-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_uneq['Data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "recent-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 1)\n",
      "(32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_uneq.at[i, 'Data'].shape)\n",
    "print(test_uneq.at[i, 'Data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "level-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uneq = np.array(train_uneq['Data'].tolist())\n",
    "y_train_uneq = np.array(train_uneq['PatN'].tolist())\n",
    "x_test_uneq = np.array(test_uneq['Data'].tolist())\n",
    "y_test_uneq = np.array(test_uneq['PatN'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "neutral-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_uneq = keras.utils.to_categorical(y_train_uneq, num_classes)\n",
    "y_test_uneq = keras.utils.to_categorical(y_test_uneq, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "suited-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        832       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 28)                14364     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 132,508\n",
      "Trainable params: 132,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_uneq = Sequential()\n",
    "inp_shape = (32, 32, 1)\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1_uneq.add(Conv2D(32, (5, 5), strides = (1,1), padding='same',\n",
    "                 input_shape=inp_shape))\n",
    "model_1_uneq.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1_uneq.add(Conv2D(32, (5, 5), strides = (2,2) , padding = 'same'))\n",
    "model_1_uneq.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1_uneq.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1_uneq.add(Dropout(0.25))\n",
    "\n",
    "model_1_uneq.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1_uneq.add(Activation('relu'))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1_uneq.add(Flatten())\n",
    "model_1_uneq.add(Dense(512))\n",
    "model_1_uneq.add(Activation('relu'))\n",
    "model_1_uneq.add(Dropout(0.5))\n",
    "model_1_uneq.add(Dense(num_classes))\n",
    "model_1_uneq.add(Activation('softmax'))\n",
    "\n",
    "model_1_uneq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "orange-locking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 2.0029 - accuracy: 0.4190 - val_loss: 1.1044 - val_accuracy: 0.7071\n",
      "Epoch 2/15\n",
      "132/132 [==============================] - 6s 42ms/step - loss: 0.9015 - accuracy: 0.7181 - val_loss: 0.8694 - val_accuracy: 0.7664\n",
      "Epoch 3/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.5880 - accuracy: 0.8186 - val_loss: 0.7297 - val_accuracy: 0.8057\n",
      "Epoch 4/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.4514 - accuracy: 0.8600 - val_loss: 0.6563 - val_accuracy: 0.8250\n",
      "Epoch 5/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.3737 - accuracy: 0.8805 - val_loss: 0.6303 - val_accuracy: 0.8293\n",
      "Epoch 6/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.2900 - accuracy: 0.9014 - val_loss: 0.6361 - val_accuracy: 0.8379\n",
      "Epoch 7/15\n",
      "132/132 [==============================] - 6s 43ms/step - loss: 0.2501 - accuracy: 0.9207 - val_loss: 0.6162 - val_accuracy: 0.8579\n",
      "Epoch 8/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.2184 - accuracy: 0.9310 - val_loss: 0.5975 - val_accuracy: 0.8579\n",
      "Epoch 9/15\n",
      "132/132 [==============================] - 6s 43ms/step - loss: 0.1868 - accuracy: 0.9390 - val_loss: 0.5244 - val_accuracy: 0.8636\n",
      "Epoch 10/15\n",
      "132/132 [==============================] - 6s 43ms/step - loss: 0.1589 - accuracy: 0.9488 - val_loss: 0.5773 - val_accuracy: 0.8579\n",
      "Epoch 11/15\n",
      "132/132 [==============================] - 6s 43ms/step - loss: 0.1347 - accuracy: 0.9562 - val_loss: 0.5505 - val_accuracy: 0.8664\n",
      "Epoch 12/15\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 0.1230 - accuracy: 0.9567 - val_loss: 0.6475 - val_accuracy: 0.8579\n",
      "Epoch 13/15\n",
      "132/132 [==============================] - 6s 45ms/step - loss: 0.1108 - accuracy: 0.9626 - val_loss: 0.5940 - val_accuracy: 0.8629\n",
      "Epoch 14/15\n",
      "132/132 [==============================] - 6s 46ms/step - loss: 0.1006 - accuracy: 0.9681 - val_loss: 0.6626 - val_accuracy: 0.8614\n",
      "Epoch 15/15\n",
      "132/132 [==============================] - 6s 45ms/step - loss: 0.0944 - accuracy: 0.9686 - val_loss: 0.5529 - val_accuracy: 0.8593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b3b84b648>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1_uneq.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1_uneq.fit(x_train_uneq, y_train_uneq,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test_uneq, y_test_uneq),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "overall-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_uneq_fully = x_train_uneq.reshape(4200, 1024)\n",
    "y_train_uneq_fully = y_train_uneq.reshape(4200, 28)\n",
    "x_test_uneq_fully = x_test_uneq.reshape(1400, 1024)\n",
    "y_test_uneq_fully = y_test_uneq.reshape(1400, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "incredible-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 28)                28700     \n",
      "=================================================================\n",
      "Total params: 3,177,500\n",
      "Trainable params: 3,177,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2_uneq = Sequential()\n",
    "model_2_uneq.add(Dense(1024, input_shape=(1024,), activation=\"relu\"))\n",
    "model_2_uneq.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2_uneq.add(Dense(1024,  activation=\"relu\"))\n",
    "model_2_uneq.add(Dense(28, activation=\"softmax\"))\n",
    "\n",
    "model_2_uneq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ordered-intent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 3s 21ms/step - loss: 3.2494 - accuracy: 0.0871 - val_loss: 3.1357 - val_accuracy: 0.2021\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 2.9918 - accuracy: 0.3929 - val_loss: 2.8807 - val_accuracy: 0.4807\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 2.6667 - accuracy: 0.6145 - val_loss: 2.5366 - val_accuracy: 0.5964\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 2.2401 - accuracy: 0.7112 - val_loss: 2.1130 - val_accuracy: 0.6529\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 1.7712 - accuracy: 0.7576 - val_loss: 1.7080 - val_accuracy: 0.6979\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 1.3701 - accuracy: 0.8005 - val_loss: 1.4120 - val_accuracy: 0.7207\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 1.0855 - accuracy: 0.8190 - val_loss: 1.2208 - val_accuracy: 0.7271\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.8947 - accuracy: 0.8374 - val_loss: 1.0919 - val_accuracy: 0.7457\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.7670 - accuracy: 0.8493 - val_loss: 1.0118 - val_accuracy: 0.7421\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.6774 - accuracy: 0.8612 - val_loss: 0.9491 - val_accuracy: 0.7614\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.6060 - accuracy: 0.8714 - val_loss: 0.9127 - val_accuracy: 0.7557\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.5528 - accuracy: 0.8748 - val_loss: 0.8797 - val_accuracy: 0.7664\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.5086 - accuracy: 0.8812 - val_loss: 0.8533 - val_accuracy: 0.7679\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.4733 - accuracy: 0.8895 - val_loss: 0.8191 - val_accuracy: 0.7814\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.4419 - accuracy: 0.8940 - val_loss: 0.8212 - val_accuracy: 0.7750\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.4150 - accuracy: 0.9000 - val_loss: 0.7947 - val_accuracy: 0.7793\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.3894 - accuracy: 0.9067 - val_loss: 0.7875 - val_accuracy: 0.7850\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3696 - accuracy: 0.9121 - val_loss: 0.7713 - val_accuracy: 0.7921\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.3506 - accuracy: 0.9160 - val_loss: 0.7706 - val_accuracy: 0.7879\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3323 - accuracy: 0.9193 - val_loss: 0.7526 - val_accuracy: 0.7893\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3182 - accuracy: 0.9240 - val_loss: 0.7491 - val_accuracy: 0.7907\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.3043 - accuracy: 0.9276 - val_loss: 0.7415 - val_accuracy: 0.7914\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2909 - accuracy: 0.9324 - val_loss: 0.7366 - val_accuracy: 0.7900\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2785 - accuracy: 0.9336 - val_loss: 0.7408 - val_accuracy: 0.7979\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2672 - accuracy: 0.9381 - val_loss: 0.7272 - val_accuracy: 0.7979\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2568 - accuracy: 0.9386 - val_loss: 0.7371 - val_accuracy: 0.7971\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2461 - accuracy: 0.9410 - val_loss: 0.7283 - val_accuracy: 0.8007\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2378 - accuracy: 0.9476 - val_loss: 0.7235 - val_accuracy: 0.8021\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2286 - accuracy: 0.9467 - val_loss: 0.7249 - val_accuracy: 0.8029\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2207 - accuracy: 0.9500 - val_loss: 0.7331 - val_accuracy: 0.7950\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.2141 - accuracy: 0.9514 - val_loss: 0.7226 - val_accuracy: 0.8043\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.2056 - accuracy: 0.9550 - val_loss: 0.7264 - val_accuracy: 0.8050\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1985 - accuracy: 0.9576 - val_loss: 0.7089 - val_accuracy: 0.8029\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1923 - accuracy: 0.9562 - val_loss: 0.7022 - val_accuracy: 0.8136\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1845 - accuracy: 0.9605 - val_loss: 0.7060 - val_accuracy: 0.8064\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1797 - accuracy: 0.9626 - val_loss: 0.7134 - val_accuracy: 0.8079\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 2s 19ms/step - loss: 0.1741 - accuracy: 0.9619 - val_loss: 0.7183 - val_accuracy: 0.8071\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1676 - accuracy: 0.9640 - val_loss: 0.7080 - val_accuracy: 0.8057\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1629 - accuracy: 0.9676 - val_loss: 0.7104 - val_accuracy: 0.8029\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1576 - accuracy: 0.9693 - val_loss: 0.7115 - val_accuracy: 0.8093\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1519 - accuracy: 0.9688 - val_loss: 0.7169 - val_accuracy: 0.8043\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1469 - accuracy: 0.9726 - val_loss: 0.7086 - val_accuracy: 0.8071\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1438 - accuracy: 0.9717 - val_loss: 0.7194 - val_accuracy: 0.8093\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1395 - accuracy: 0.9729 - val_loss: 0.7155 - val_accuracy: 0.8064\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1351 - accuracy: 0.9748 - val_loss: 0.7196 - val_accuracy: 0.8064\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1312 - accuracy: 0.9771 - val_loss: 0.7246 - val_accuracy: 0.8086\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1272 - accuracy: 0.9790 - val_loss: 0.7213 - val_accuracy: 0.8093\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1232 - accuracy: 0.9795 - val_loss: 0.7336 - val_accuracy: 0.8029\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 3s 20ms/step - loss: 0.1204 - accuracy: 0.9805 - val_loss: 0.7162 - val_accuracy: 0.8071\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 3s 19ms/step - loss: 0.1163 - accuracy: 0.9829 - val_loss: 0.7132 - val_accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "model_2_uneq.compile(SGD(lr = .003), \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_2_uneq.fit(x_train_uneq_fully, y_train_uneq_fully, validation_data=(x_test_uneq_fully, y_test_uneq_fully), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-reader",
   "metadata": {},
   "source": [
    "Here the model with CNN are more accurate than the fully connected model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-wales",
   "metadata": {},
   "source": [
    " # Final comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-helena",
   "metadata": {},
   "source": [
    "As we see from the models the best model is CNN model. This type of models is more specialized for images, although the fully connected prediction almost approached the CNN, this type of models required to many parameters for training. For this type of tasks(1 dimension of color channel) Fully connected neural nets is fast enough, but with RGB pictures it can take so much time for training model. Also, the performance of CNN can be increased, if we make this neural net deeper(i used simple version of CNN) and reach the number of parameters of Fully con. neural net."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
